{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "import bayesflow as bf\n",
        "import pickle\n",
        "import EZ2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dYn4egPEC1d"
      },
      "outputs": [],
      "source": [
        "def prior():\n",
        "  params = {}\n",
        "\n",
        "  # Drift rates v toward left and right responses\n",
        "  params['vL'] = np.random.uniform(0.1, 6.0)\n",
        "  params['vR'] = np.random.uniform(0.1, 6.0)\n",
        "\n",
        "  # Boundary separation a\n",
        "  params['a'] = np.random.uniform(0.3, 4.0)\n",
        "\n",
        "  # Relative starting point z\n",
        "  params['z'] = np.random.uniform(0.1, 0.9)\n",
        "\n",
        "  # Non-decision times ter for left and right responses (in seconds)\n",
        "  params['terL'] = np.random.uniform(0.1, 1.0)\n",
        "  params['terR'] = np.random.uniform(0.1, 1.0)\n",
        "\n",
        "  return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhLa63dNg1SU"
      },
      "outputs": [],
      "source": [
        "# Helper functions\n",
        "\n",
        "def _p_bottom_safe(nu, z, a, s=0.1):\n",
        "    \"\"\"Bottom-hit probability with a fallback for near-zero nu.\"\"\"\n",
        "    if abs(nu) < 1e-12:\n",
        "        return (a - z) / a\n",
        "    s2 = s * s\n",
        "    num = np.exp(-2*a*nu/s2) - np.exp(-2*z*nu/s2)\n",
        "    den = np.exp(-2*a*nu/s2) - 1.0\n",
        "    return float(np.clip(num / den, 0.0, 1.0))\n",
        "\n",
        "def _safe_rddexit(size, nu, z, a, top_boundary):\n",
        "    \"\"\"Call EZ2.rddexit and always return a list (even for size==1).\"\"\"\n",
        "    if size <= 0:\n",
        "        return []\n",
        "    arr = EZ2.rddexit(size, nu, z, a, top_boundary=top_boundary)\n",
        "    if np.isscalar(arr):\n",
        "        return [float(arr)]\n",
        "    return [float(x) for x in np.asarray(arr).ravel()]\n",
        "\n",
        "def _sample_times(size, nu, z, a, s=0.1, rng=None):\n",
        "    \"\"\"Safer equivalent of rddexitj using robust fallbacks.\"\"\"\n",
        "    if rng is None:\n",
        "        rng = np.random.default_rng()\n",
        "    p0 = _p_bottom_safe(nu, z, a, s=s)\n",
        "    n_bottom = rng.binomial(size, p0)\n",
        "    n_top = size - n_bottom\n",
        "    et_bottom = _safe_rddexit(n_bottom, nu, z, a, top_boundary=False)\n",
        "    et_top    = _safe_rddexit(n_top,    nu, z, a, top_boundary=True)\n",
        "    return et_bottom, et_top\n",
        "\n",
        "# Forward model\n",
        "\n",
        "def forward_model_ez2(\n",
        "    vL, vR, a, z, terL, terR, n_trials=200, rng=None,\n",
        "    rt_transform=\"log1p\" # \"log1p\" or \"none\"\n",
        "):\n",
        "    if rng is None:\n",
        "        rng = np.random.default_rng()\n",
        "\n",
        "    # scale to s=0.1 used in functions EZ2\n",
        "    c = 0.1\n",
        "    vL_ez, vR_ez = float(vL)*c, float(vR)*c\n",
        "    a_ez = float(a)*c\n",
        "\n",
        "    # converting relative z to absolute z\n",
        "    z_abs = float(z) * a_ez\n",
        "    eps = 1e-9 * a_ez\n",
        "    z_abs = min(max(z_abs, eps), a_ez - eps)\n",
        "\n",
        "    nA = n_trials // 2\n",
        "    nB = n_trials - nA\n",
        "\n",
        "    # A condition (Left correct): top->Left(0), bottom->Right(1)\n",
        "    et_b_A, et_t_A = _sample_times(nA, vL_ez, z_abs, a_ez, s=0.1, rng=rng)\n",
        "    et_b_A = np.asarray(et_b_A, dtype=np.float64)\n",
        "    et_t_A = np.asarray(et_t_A, dtype=np.float64)\n",
        "    nAb, nAt = et_b_A.size, et_t_A.size\n",
        "    dts_A = np.empty(nA, dtype=np.float64); dts_A[:nAb] = et_b_A; dts_A[nAb:] = et_t_A\n",
        "    choices_A = np.empty(nA, dtype=np.int64); choices_A[:nAb] = 1; choices_A[nAb:] = 0\n",
        "    correct_A = np.empty(nA, dtype=np.int64); correct_A[:nAb] = 0; correct_A[nAb:] = 1\n",
        "    stim_A = np.zeros(nA, dtype=np.int64)\n",
        "\n",
        "    # B condition (Right correct): top->Right(1), bottom->Left(0)\n",
        "    et_b_B, et_t_B = _sample_times(nB, vR_ez, a_ez - z_abs, a_ez, s=0.1, rng=rng)\n",
        "    et_b_B = np.asarray(et_b_B, dtype=np.float64)\n",
        "    et_t_B = np.asarray(et_t_B, dtype=np.float64)\n",
        "    nBb, nBt = et_b_B.size, et_t_B.size\n",
        "    dts_B = np.empty(nB, dtype=np.float64); dts_B[:nBb] = et_b_B; dts_B[nBb:] = et_t_B\n",
        "    choices_B = np.empty(nB, dtype=np.int64); choices_B[:nBb] = 0; choices_B[nBb:] = 1\n",
        "    correct_B = np.empty(nB, dtype=np.int64); correct_B[:nBb] = 0; correct_B[nBb:] = 1\n",
        "    stim_B = np.ones(nB, dtype=np.int64)\n",
        "\n",
        "    dts = np.concatenate([dts_A, dts_B])\n",
        "    choices = np.concatenate([choices_A, choices_B])\n",
        "    correct = np.concatenate([correct_A, correct_B])\n",
        "    stimulus = np.concatenate([stim_A, stim_B])\n",
        "\n",
        "    perm = rng.permutation(n_trials)\n",
        "    dts, choices, correct, stimulus = dts[perm], choices[perm], correct[perm], stimulus[perm]\n",
        "\n",
        "    # Add ter and optional log-transform\n",
        "    rts = dts + np.where(choices == 0, terL, terR)\n",
        "    if rt_transform == \"log1p\":\n",
        "        rts = np.log1p(rts)\n",
        "\n",
        "    return {\n",
        "        \"rts\": rts.astype(np.float32),\n",
        "        \"choices\": choices,\n",
        "        \"stimulus\": stimulus,\n",
        "        \"correct\": correct,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTT7wZ8eEeIm"
      },
      "outputs": [],
      "source": [
        "simulator = bf.make_simulator([prior, forward_model_ez2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMW85RhEg1SW"
      },
      "outputs": [],
      "source": [
        "param_names = ['vL', 'vR', 'a', 'z', 'terL', 'terR']\n",
        "data_names = ['rts', 'stimulus', 'choices']  # Removed 'correct' for redundancy of information\n",
        "# 'correct' can be derived from 'choices' and 'stimulus' so we it's not needed\n",
        "\n",
        "adapter = (\n",
        "    bf.adapters.Adapter()\n",
        "    .keep(param_names + data_names)\n",
        "    .to_array()\n",
        "    .convert_dtype(\"float64\", \"float32\")\n",
        "    .expand_dims(\"rts\", axis=-1)\n",
        "    .expand_dims(\"choices\", axis=-1)\n",
        "    .expand_dims(\"stimulus\", axis=-1)\n",
        "    .concatenate(param_names, into=\"inference_variables\")\n",
        "    .concatenate(data_names, into=\"summary_variables\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Bz4MLcY6vFn"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, BackupAndRestore\n",
        "\n",
        "es = EarlyStopping(\n",
        "    monitor=\"loss\",\n",
        "    mode=\"min\",\n",
        "    min_delta=0.01,\n",
        "    patience=20,\n",
        "    restore_best_weights=True,\n",
        "    start_from_epoch=5\n",
        ")\n",
        "\n",
        "callbacks = [\n",
        "    BackupAndRestore(backup_dir=\"./standard_backup\"),\n",
        "    ModelCheckpoint(\"standard_model_ckpt.keras\", monitor=\"loss\", save_best_only=True),\n",
        "    es\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpV3BvxlTx_4"
      },
      "outputs": [],
      "source": [
        "from bayesflow.networks import CouplingFlow, DeepSet\n",
        "from bayesflow.workflows import BasicWorkflow\n",
        "\n",
        "summary_net = DeepSet(\n",
        "    summary_dim=16,\n",
        "    dropout=0.1\n",
        ")\n",
        "\n",
        "flow = CouplingFlow(\n",
        "    num_coupling_layers=6,\n",
        "    hidden_units=[128, 128],\n",
        "    coupling_type=\"spline\",\n",
        "    batch_norm=True,\n",
        "    dropout=0.05,\n",
        "    tail_bound=6.0\n",
        ")\n",
        "\n",
        "wf = BasicWorkflow(\n",
        "    simulator=simulator,\n",
        "    adapter=adapter,\n",
        "    summary_network=summary_net,\n",
        "    inference_network=flow,\n",
        "    inference_variables=[\"inference_variables\"],\n",
        "    summary_variables=[\"summary_variables\"]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d93IFhXE_B1",
        "outputId": "b90460c2-20cf-44b1-cc09-3ff1073d6139"
      },
      "outputs": [],
      "source": [
        "history = wf.fit_online(\n",
        "    epochs = 2000,\n",
        "    num_batches_per_epoch = 150,\n",
        "    batch_size = 64,\n",
        "    callbacks = callbacks\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "out = Path.cwd() / \"standard_model.keras\"\n",
        "out.parent.mkdir(parents=True, exist_ok=True)\n",
        "wf.approximator.save(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "pOox6_LoFDre",
        "outputId": "3e8159ac-8047-4d15-c96a-33b98524121f"
      },
      "outputs": [],
      "source": [
        "f = bf.diagnostics.plots.loss(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "xyQuDyiSOnko",
        "outputId": "61c3a8e3-dbb4-4599-fcfa-a1aa1fa87e15"
      },
      "outputs": [],
      "source": [
        "num_samples = 1000\n",
        "\n",
        "# Simulate validation data (unseen during training)\n",
        "val_sims = simulator.sample(200)\n",
        "\n",
        "# Obtain num_samples samples of the parameter posterior for every validation dataset\n",
        "post_draws = wf.sample(conditions=val_sims, num_samples=num_samples)\n",
        "\n",
        "f = bf.diagnostics.plots.recovery(\n",
        "    estimates=post_draws,\n",
        "    targets=val_sims,\n",
        "    variable_names=param_names\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
