{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42543f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import bayesflow as bf\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import EZ2\n",
    "from keras import layers\n",
    "from keras.saving import register_keras_serializable\n",
    "\n",
    "@register_keras_serializable()\n",
    "class IdentitySummaryNet(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc42230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior():\n",
    "  params = {}\n",
    "\n",
    "  # Drift rates v toward left and right responses\n",
    "  params['vL'] = np.random.uniform(0.1, 6.0)\n",
    "  params['vR'] = np.random.uniform(0.1, 6.0)\n",
    "\n",
    "  # Boundary separation a\n",
    "  params['a'] = np.random.uniform(0.3, 4.0)\n",
    "\n",
    "  # Relative starting point z\n",
    "  params['z'] = np.random.uniform(0.1, 0.9)\n",
    "\n",
    "  # Non-decision times ter for left and right responses (in seconds)\n",
    "  params['terL'] = np.random.uniform(0.1, 1.0)\n",
    "  params['terR'] = np.random.uniform(0.1, 1.0)\n",
    "\n",
    "  return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4621ad34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_model_ez(vL, vR, a, z, terL, terR):\n",
    "    z_abs = z * a # convert relative z to absolute z\n",
    "\n",
    "    mrtR = EZ2.cmrt(vR, z_abs, a, s=1) + terR\n",
    "    vrtR = EZ2.cvrt(vR, z_abs, a, s=1)\n",
    "    peR  = EZ2.pe(vR, z_abs, a, s=1)\n",
    "\n",
    "    mrtL = EZ2.cmrt(vL, a - z_abs, a, s=1) + terL\n",
    "    vrtL = EZ2.cvrt(vL, a - z_abs, a, s=1)\n",
    "    peL  = EZ2.pe(vL, a - z_abs, a, s=1)\n",
    "\n",
    "    return {\n",
    "        'mrtL': mrtL,\n",
    "        'vrtL': vrtL,\n",
    "        'peL':  peL,\n",
    "        'mrtR': mrtR,\n",
    "        'vrtR': vrtR,\n",
    "        'peR':  peR\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83e9554",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator = bf.make_simulator([prior, forward_model_ez])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f68bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_names = ['vL', 'vR', 'a', 'z', 'terL', 'terR']\n",
    "data_names = ['mrtL', 'vrtL', 'peL', 'mrtR', 'vrtR', 'peR']\n",
    "\n",
    "adapter = (\n",
    "    bf.adapters.Adapter()\n",
    "    .keep(param_names + data_names)\n",
    "    .to_array()\n",
    "    .convert_dtype(\"float64\", \"float32\")\n",
    "    .concatenate(param_names, into=\"inference_variables\")\n",
    "    .concatenate(data_names, into=\"summary_variables\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00615c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model, Input\n",
    "from keras.layers import Layer\n",
    "\n",
    "# Define a simple identity summary network\n",
    "# Since the training data already consists of summary statistics,\n",
    "# we pass them via an identity network.\n",
    "class IdentitySummaryNet(Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs\n",
    "\n",
    "    def compute_metrics(self, inputs, stage=None):\n",
    "        return {\"outputs\": self(inputs)}\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "summary_net = IdentitySummaryNet()\n",
    "\n",
    "from bayesflow.networks import CouplingFlow\n",
    "from bayesflow.workflows import BasicWorkflow\n",
    "\n",
    "flow = CouplingFlow(\n",
    "    num_coupling_layers=6,\n",
    "    hidden_units=[128, 128],\n",
    "    coupling_type=\"spline\",\n",
    "    batch_norm=True,\n",
    "    dropout=0.05,\n",
    "    tail_bound=5.0\n",
    ")\n",
    "\n",
    "wf = BasicWorkflow(\n",
    "    simulator=simulator,\n",
    "    adapter=adapter,\n",
    "    summary_network=summary_net,\n",
    "    inference_network=flow,\n",
    "    inference_variables=[\"inference_variables\"],\n",
    "    summary_variables=[\"summary_variables\"],\n",
    "    standardize=[\"summary_variables\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31a43f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_approx = keras.models.load_model(\n",
    "  \"C:/Users/emils/Documents/uni/M_Thesis/diffusion-bayesflow/scripts/ezbf_model.keras\",\n",
    "  custom_objects={\"IdentitySummaryNet\": IdentitySummaryNet}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ce30f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wf = BasicWorkflow(\n",
    "    simulator=simulator,\n",
    "    adapter=adapter,\n",
    "    summary_network=summary_net,\n",
    "    inference_network=flow,\n",
    "    inference_variables=[\"inference_variables\"],\n",
    "    summary_variables=[\"summary_variables\"],\n",
    "    standardize=[\"summary_variables\"]\n",
    ")\n",
    "\n",
    "wf.approximator = loaded_approx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9d1ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definining a new prior to be used to generate the simulated test data\n",
    "# This prior is based on realistic, empirical parameter distributions described\n",
    "# in Tran et al. (2021)\n",
    "\n",
    "def sample_trunc_t(df, loc, scale, lower, upper):\n",
    "    while True:\n",
    "        x = np.random.standard_t(df=df)\n",
    "        y = loc + scale * x\n",
    "        if lower <= y <= upper:\n",
    "            return y\n",
    "\n",
    "def plausible_prior():\n",
    "    # Drift rates: Normal(1.76, 1.51) truncated to ≥ 0.2\n",
    "    while True:\n",
    "        vL = np.random.normal(loc=1.76, scale=1.51)\n",
    "        vR = np.random.normal(loc=1.76, scale=1.51)\n",
    "        if vL >= 0.2 and vR >= 0.2:\n",
    "            break\n",
    "\n",
    "    # Boundary separation: Gamma(11.69, 0.12) capped at 4\n",
    "    a = np.random.gamma(shape=11.69, scale=0.12)\n",
    "    a = min(a, 4)\n",
    "\n",
    "    # Starting point: Truncated Student-T in [0, 1]\n",
    "    z = sample_trunc_t(df=1.85, loc=0.5, scale=0.1, lower=0.0, upper=1.0)\n",
    "\n",
    "    # Non-decision times: Truncated Student-T ≥ 0\n",
    "    while True:\n",
    "        terL = sample_trunc_t(df=1.32, loc=0.44, scale=0.08, lower=0.0, upper=np.inf)\n",
    "        terR = sample_trunc_t(df=1.32, loc=0.44, scale=0.08, lower=0.0, upper=np.inf)\n",
    "        if terL >= 0 and terR >= 0:\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        \"vL\": vL,\n",
    "        \"vR\": vR,\n",
    "        \"a\": a,\n",
    "        \"z\": z,\n",
    "        \"terL\": terL,\n",
    "        \"terR\": terR\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d919d5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def _p_bottom_safe(nu, z, a, s=0.1):\n",
    "    \"\"\"Bottom-hit probability with a fallback for near-zero nu.\"\"\"\n",
    "    if abs(nu) < 1e-12:\n",
    "        return (a - z) / a\n",
    "    s2 = s * s\n",
    "    num = np.exp(-2*a*nu/s2) - np.exp(-2*z*nu/s2)\n",
    "    den = np.exp(-2*a*nu/s2) - 1.0\n",
    "    return float(np.clip(num / den, 0.0, 1.0))\n",
    "\n",
    "def _safe_rddexit(size, nu, z, a, top_boundary):\n",
    "    \"\"\"Call EZ2.rddexit and always return a list (even for size==1).\"\"\"\n",
    "    if size <= 0:\n",
    "        return []\n",
    "    arr = EZ2.rddexit(size, nu, z, a, top_boundary=top_boundary)\n",
    "    if np.isscalar(arr):\n",
    "        return [float(arr)]\n",
    "    return [float(x) for x in np.asarray(arr).ravel()]\n",
    "\n",
    "def _sample_times(size, nu, z, a, s=0.1, rng=None):\n",
    "    \"\"\"Safer equivalent of rddexitj using robust fallbacks.\"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "    p0 = _p_bottom_safe(nu, z, a, s=s)\n",
    "    n_bottom = rng.binomial(size, p0)\n",
    "    n_top = size - n_bottom\n",
    "    et_bottom = _safe_rddexit(n_bottom, nu, z, a, top_boundary=False)\n",
    "    et_top    = _safe_rddexit(n_top,    nu, z, a, top_boundary=True)\n",
    "    return et_bottom, et_top\n",
    "\n",
    "# Forward model\n",
    "\n",
    "def forward_model_ez2(\n",
    "    vL, vR, a, z, terL, terR, n_trials=200, rng=None,\n",
    "    rt_transform=\"log1p\" # \"log1p\" or \"none\"\n",
    "):\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    # scale to s=0.1 used in functions EZ2\n",
    "    c = 0.1\n",
    "    vL_ez, vR_ez = float(vL)*c, float(vR)*c\n",
    "    a_ez = float(a)*c\n",
    "\n",
    "    # converting relative z to absolute z\n",
    "    z_abs = float(z) * a_ez\n",
    "    eps = 1e-9 * a_ez\n",
    "    z_abs = min(max(z_abs, eps), a_ez - eps)\n",
    "\n",
    "    nA = n_trials // 2\n",
    "    nB = n_trials - nA\n",
    "\n",
    "    # A condition (Left correct): top->Left(0), bottom->Right(1)\n",
    "    et_b_A, et_t_A = _sample_times(nA, vL_ez, z_abs, a_ez, s=0.1, rng=rng)\n",
    "    et_b_A = np.asarray(et_b_A, dtype=np.float64)\n",
    "    et_t_A = np.asarray(et_t_A, dtype=np.float64)\n",
    "    nAb, nAt = et_b_A.size, et_t_A.size\n",
    "    dts_A = np.empty(nA, dtype=np.float64); dts_A[:nAb] = et_b_A; dts_A[nAb:] = et_t_A\n",
    "    choices_A = np.empty(nA, dtype=np.int64); choices_A[:nAb] = 1; choices_A[nAb:] = 0\n",
    "    correct_A = np.empty(nA, dtype=np.int64); correct_A[:nAb] = 0; correct_A[nAb:] = 1\n",
    "    stim_A = np.zeros(nA, dtype=np.int64)\n",
    "\n",
    "    # B condition (Right correct): top->Right(1), bottom->Left(0)\n",
    "    et_b_B, et_t_B = _sample_times(nB, vR_ez, a_ez - z_abs, a_ez, s=0.1, rng=rng)\n",
    "    et_b_B = np.asarray(et_b_B, dtype=np.float64)\n",
    "    et_t_B = np.asarray(et_t_B, dtype=np.float64)\n",
    "    nBb, nBt = et_b_B.size, et_t_B.size\n",
    "    dts_B = np.empty(nB, dtype=np.float64); dts_B[:nBb] = et_b_B; dts_B[nBb:] = et_t_B\n",
    "    choices_B = np.empty(nB, dtype=np.int64); choices_B[:nBb] = 0; choices_B[nBb:] = 1\n",
    "    correct_B = np.empty(nB, dtype=np.int64); correct_B[:nBb] = 0; correct_B[nBb:] = 1\n",
    "    stim_B = np.ones(nB, dtype=np.int64)\n",
    "\n",
    "    dts = np.concatenate([dts_A, dts_B])\n",
    "    choices = np.concatenate([choices_A, choices_B])\n",
    "    correct = np.concatenate([correct_A, correct_B])\n",
    "    stimulus = np.concatenate([stim_A, stim_B])\n",
    "\n",
    "    perm = rng.permutation(n_trials)\n",
    "    dts, choices, correct, stimulus = dts[perm], choices[perm], correct[perm], stimulus[perm]\n",
    "\n",
    "    # Add ter and optional log-transform\n",
    "    rts = dts + np.where(choices == 0, terL, terR)\n",
    "    if rt_transform == \"log1p\":\n",
    "        rts = np.log1p(rts)\n",
    "\n",
    "    return {\n",
    "        \"rts\": rts.astype(np.float32),\n",
    "        \"choices\": choices,\n",
    "        \"stimulus\": stimulus,\n",
    "        \"correct\": correct,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e4fcdc",
   "metadata": {},
   "source": [
    "The code cell below generates the set of thetas and test datasets used in both model notebooks. A skipping logic and cycling over candidate parameter sets are implemented due to issues with unstable data generation on particular sets of parameters. On most runs of the notebook, 100 of the 100 candidate thetas were valid, with at most 2 or 3 sets needing to be skipped.\n",
    "\n",
    "Moreover, rather than using the EZ-BF's forward model that obtains the EZ2 population summaries using the EZ2 functions from the parameters, the present approach simulates \"raw\" 2AFC data using the standard model's forward model and the sample summaries are calculated for use in the EZ-BF model. This aimed to make the testing of the model more realistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8b29fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "n_samples = 500\n",
    "max_candidates = 1000\n",
    "n_trials = 400\n",
    "np.random.seed(123)\n",
    "\n",
    "accepted_thetas = []\n",
    "skip_log = [] # Skipping logic is implemented due to model instability on certain parameter combinations\n",
    "true_params = []\n",
    "obs_data = []\n",
    "post_samples = []\n",
    "\n",
    "for i in range(max_candidates):\n",
    "    if len(accepted_thetas) >= N:\n",
    "        break\n",
    "\n",
    "    theta = plausible_prior()\n",
    "\n",
    "    try:\n",
    "        # Simulating \"raw\" 2AFC data using the standard model's forward model\n",
    "        sim = forward_model_ez2(**theta, n_trials=n_trials)\n",
    "        rts_log = sim[\"rts\"]                 \n",
    "        rts = np.expm1(rts_log)   # inverting the log-transformation on the RTs\n",
    "        choices = sim[\"choices\"]\n",
    "        stimulus = sim[\"stimulus\"]\n",
    "        correct = sim[\"correct\"]\n",
    "\n",
    "        if not (np.isfinite(rts).all() and np.isfinite(choices).all()):\n",
    "            print(f\"[{i}] Skipping: Non-finite RTs or choices.\")\n",
    "            continue\n",
    "\n",
    "        # Store raw observation\n",
    "        obs_data.append(np.stack([rts, stimulus, choices, correct], axis=-1))\n",
    "\n",
    "        # Computing summary stats from raw data\n",
    "        stats = {}\n",
    "        for stim_val, label in zip([0, 1], ['L', 'R']):\n",
    "            mask = stimulus == stim_val\n",
    "            rts_stim = rts[mask]\n",
    "            choices_stim = choices[mask]\n",
    "            correct_stim = correct[mask]\n",
    "\n",
    "            if len(rts_stim) < 3:\n",
    "                skip_log.append({\n",
    "                      \"index\": i,\n",
    "                      \"reason\": f\"Too few trials for stimulus {label}\",\n",
    "                      \"theta\": theta,\n",
    "                      \"summary_inputs\": {\n",
    "                        \"rts\": rts_stim.tolist(),\n",
    "                        \"choices\": choices_stim.tolist(),\n",
    "                        \"correct\": correct_stim.tolist(),\n",
    "                        }\n",
    "                })\n",
    "                raise ValueError()\n",
    "            \n",
    "            stats[f'mrt{label}'] = np.mean(rts_stim)\n",
    "            stats[f'vrt{label}'] = np.var(rts_stim, ddof=1)\n",
    "            stats[f'pe{label}'] = 1.0 - np.mean(correct_stim)\n",
    "\n",
    "        if not np.isfinite(list(stats.values())).all():\n",
    "            obs_data.pop()\n",
    "            skip_log.append({\n",
    "                \"index\": i,\n",
    "                \"reason\": \"Summary stats contain non-finite values\",\n",
    "                \"theta\": theta,\n",
    "                \"summary_stats\": stats\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Inference\n",
    "        conditions = {\n",
    "            f\"mrt{s}\": np.array([[stats[f\"mrt{s}\"]]], dtype=np.float32)\n",
    "            for s in ['L', 'R']\n",
    "        } | {\n",
    "            f\"vrt{s}\": np.array([[stats[f\"vrt{s}\"]]], dtype=np.float32)\n",
    "            for s in ['L', 'R']\n",
    "        } | {\n",
    "            f\"pe{s}\": np.array([[stats[f\"pe{s}\"]]], dtype=np.float32)\n",
    "            for s in ['L', 'R']\n",
    "        }\n",
    "\n",
    "        samples = wf.sample(conditions=conditions, num_samples=n_samples, to_numpy=True)\n",
    "\n",
    "        post_array = np.stack([\n",
    "            samples[\"vL\"], samples[\"vR\"],\n",
    "            samples[\"a\"],  samples[\"z\"],\n",
    "            samples[\"terL\"], samples[\"terR\"]\n",
    "        ], axis=-1)\n",
    "\n",
    "        post_array = np.squeeze(post_array, axis=(0, 2))  # (n_samples, 6)\n",
    "\n",
    "        if not np.isfinite(post_array).all():\n",
    "            obs_data.pop()\n",
    "            skip_log.append({\n",
    "                \"index\": i,\n",
    "                \"reason\": \"Posterior samples contain NaNs\",\n",
    "                \"theta\": theta,\n",
    "                \"summary_stats\": stats\n",
    "            })\n",
    "            continue\n",
    "\n",
    "    except Exception as e:\n",
    "        if obs_data:\n",
    "            obs_data.pop()\n",
    "        skip_log.append({\n",
    "            \"index\": i,\n",
    "            \"reason\": f\"Exception — {str(e)}\",\n",
    "            \"theta\": theta\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    accepted_thetas.append(theta)\n",
    "    true_params.append([theta[k] for k in [\"vL\", \"vR\", \"a\", \"z\", \"terL\", \"terR\"]])\n",
    "    post_samples.append(post_array)\n",
    "\n",
    "print(f\"\\nAccepted {len(accepted_thetas)} clean datasets from {i} candidates.\")\n",
    "\n",
    "# Convert to arrays\n",
    "true_params = np.array(true_params)   # (N, 6)\n",
    "obs_data = np.array(obs_data)         # (N, n_trials, 4)\n",
    "post_samples = np.array(post_samples) # (N, n_samples, 6)\n",
    "\n",
    "\n",
    "# Storing these sets for use in bf_results.ipynb\n",
    "# These sets were generated to ensure the same thetas are used for both models to obtain comparable results\n",
    "with open(\"set_thetas.pkl\", \"wb\") as f:\n",
    "    pickle.dump(accepted_thetas, f)\n",
    "\n",
    "with open(\"set_test_data.npy\", \"wb\") as f:\n",
    "    np.save(f, obs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1377b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy metrics\n",
    "\n",
    "# Optional: toggle which point estimate is used for calculating errors\n",
    "POINT_EST = \"mean\" # \"mean\" or \"median\"\n",
    "\n",
    "param_names = ['vL', 'vR', 'a', 'z', 'terL', 'terR']\n",
    "\n",
    "records = []\n",
    "for i in range(true_params.shape[0]):\n",
    "    for j, name in enumerate(param_names):\n",
    "        true_val = float(true_params[i, j])\n",
    "        samples  = post_samples[i, :, j]\n",
    "\n",
    "        # Posterior summaries\n",
    "        post_mean = float(np.mean(samples))\n",
    "        post_median = float(np.median(samples))\n",
    "        post_var = float(np.var(samples, ddof=1))\n",
    "        post_sd  = float(np.sqrt(post_var))\n",
    "\n",
    "        # Point estimate used for accuracy metrics\n",
    "        point_est = post_median if POINT_EST == \"median\" else post_mean\n",
    "\n",
    "        # Per-dataset estimation error (bias at the single-dataset level)\n",
    "        error = float(point_est - true_val)\n",
    "        se    = float(error**2)                      # squared error (per dataset)\n",
    "\n",
    "        records.append({\n",
    "            \"dataset\": i,\n",
    "            \"parameter\": name,\n",
    "            \"true_value\": true_val,\n",
    "            \"posterior_mean\": post_mean,\n",
    "            \"posterior_median\": post_median,\n",
    "            \"posterior_variance\": post_var,\n",
    "            \"posterior_sd\": post_sd,\n",
    "            \"point_estimate\": point_est,            # the estimator used for triad\n",
    "            \"error\": error,                         # will aggregate to Bias = mean(error)\n",
    "            \"mse\": se                               # per-dataset squared error; MSE = mean(mse)\n",
    "        })\n",
    "\n",
    "performance_stats_df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2de944",
   "metadata": {},
   "outputs": [],
   "source": [
    "triad = (\n",
    "    performance_stats_df\n",
    "    .groupby('parameter')\n",
    "    .agg(\n",
    "        n=('error','size'),\n",
    "        Bias=('error','mean'),\n",
    "        Variance_of_error=('error', lambda s: s.var(ddof=1)),\n",
    "        MSE=('mse','mean')\n",
    "    ).reset_index()\n",
    ")\n",
    "\n",
    "triad.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d653b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying and cleaning outliers\n",
    "thr = performance_stats_df.groupby('parameter')['error'].transform(lambda s: s.abs().quantile(0.75) + 3.0*(s.abs().quantile(0.75)-s.abs().quantile(0.25)))\n",
    "df_clean = performance_stats_df[performance_stats_df['error'].abs() <= thr].copy()\n",
    "\n",
    "is_out = ~performance_stats_df.index.isin(df_clean.index)\n",
    "outlier_counts = performance_stats_df.assign(outlier=is_out).groupby('parameter')['outlier'].agg(n_total='size', n_dropped='sum').assign(pct_dropped=lambda x: 100*x['n_dropped']/x['n_total'])\n",
    "outlier_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8ceee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "triad_clean = (\n",
    "    df_clean\n",
    "    .groupby('parameter')\n",
    "    .agg(\n",
    "        n=('error','size'),\n",
    "        Bias=('error','mean'),\n",
    "        Variance=('error', lambda s: s.var(ddof=1)),   # variance of error across datasets\n",
    "        MSE=('error', lambda s: float(np.mean(s**2)))  # mean squared error across datasets\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "triad_clean.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a9c39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing output for later model comparison\n",
    "\n",
    "MODEL_LABEL = \"ez-bf\"\n",
    "\n",
    "performance_stats_df = performance_stats_df.copy()\n",
    "performance_stats_df[\"model\"] = MODEL_LABEL\n",
    "df_clean = df_clean.copy()\n",
    "df_clean[\"model\"] = MODEL_LABEL\n",
    "\n",
    "performance_stats_df.to_csv(f\"{MODEL_LABEL}_perf_full.csv\", index=False)\n",
    "df_clean.to_csv(f\"{MODEL_LABEL}_perf_clean.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba3c6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the bias and posterior variance\n",
    "\n",
    "import seaborn as sns, matplotlib.pyplot as plt, numpy as np, pandas as pd\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "DFU = performance_stats_df.copy()\n",
    "base = performance_stats_df\n",
    "\n",
    "# counts dropped per parameter\n",
    "n_total = base.groupby(\"parameter\").size()\n",
    "n_kept  = DFU.groupby(\"parameter\").size()\n",
    "n_drop  = (n_total - n_kept).reindex(n_total.index).fillna(0).astype(int)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Panel A: estimation error\n",
    "sns.boxplot(data=DFU, x=\"parameter\", y=\"error\", ax=axes[0], showfliers=False)\n",
    "axes[0].axhline(0, ls=\"--\", c=\"gray\", lw=1)\n",
    "axes[0].set_ylim(-1.6, 1.9)\n",
    "axes[0].set_title(\"Bias by Parameter\")\n",
    "axes[0].set_xlabel(\"\"); axes[0].set_ylabel(\"Bias\")\n",
    "\n",
    "# Panel B: posterior variance\n",
    "sns.boxplot(data=DFU, x=\"parameter\", y=\"posterior_sd\", ax=axes[1], showfliers=False)\n",
    "axes[1].set_ylim(0, 0.6)\n",
    "axes[1].set_title(\"Posterior SD by Parameter\")\n",
    "axes[1].set_xlabel(\"\"); axes[1].set_ylabel(\"Posterior SD\")\n",
    "\n",
    "plt.tight_layout(rect=[0,0.05,1,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2a933a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credible intervals and coverage\n",
    "\n",
    "\n",
    "N, S, P = post_samples.shape  # N = datasets, S = samples, P = parameters\n",
    "\n",
    "ci_records = []\n",
    "\n",
    "for i in range(N):\n",
    "    for j, name in enumerate(param_names):\n",
    "        samples_ij = post_samples[i, :, j]\n",
    "        true_val = true_params[i, j]\n",
    "\n",
    "        # Compute 95% credible interval\n",
    "        lower = np.percentile(samples_ij, 2.5)\n",
    "        upper = np.percentile(samples_ij, 97.5)\n",
    "        width = upper - lower\n",
    "\n",
    "        # Coverage\n",
    "        covered = int(lower <= true_val <= upper)\n",
    "\n",
    "        ci_records.append({\n",
    "            \"dataset\": i,\n",
    "            \"parameter\": name,\n",
    "            \"true_value\": true_val,\n",
    "            \"lower_95\": lower,\n",
    "            \"upper_95\": upper,\n",
    "            \"width_95\": width,\n",
    "            \"covered\": covered\n",
    "        })\n",
    "\n",
    "ci_df = pd.DataFrame(ci_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2065574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute overall coverage per parameter\n",
    "coverage_summary = ci_df.groupby(\"parameter\")[\"covered\"].mean().reset_index()\n",
    "coverage_summary.rename(columns={\"covered\": \"coverage_rate\"}, inplace=True)\n",
    "\n",
    "# Plot distribution of credible interval widths\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=ci_df, x=\"parameter\", y=\"width_95\")\n",
    "plt.title(\"Distribution of 95% CrI Widths by Parameter\")\n",
    "plt.ylabel(\"Width of 95% CrI\")\n",
    "plt.xlabel(\"Parameter\")\n",
    "plt.ylim(0, 4)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Return the summary coverage table\n",
    "coverage_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ebff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing output for later model comparison\n",
    "\n",
    "MODEL_LABEL = \"ez-bf\"\n",
    "ci_df_model = ci_df.copy()\n",
    "ci_df_model[\"model\"] = MODEL_LABEL\n",
    "ci_df_model.to_csv(f\"{MODEL_LABEL}_ci_full.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffe5ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation-based calibration (SBC)\n",
    "\n",
    "from scipy.stats import chi2, kstest, binom\n",
    "\n",
    "N, S, P = post_samples.shape\n",
    "rng = np.random.default_rng(1312)\n",
    "\n",
    "# Randomized ranks and normalized ranks z in (0,1)\n",
    "sbc_records = []\n",
    "for j, name in enumerate(param_names):\n",
    "    for i in range(N):\n",
    "        true_val = float(true_params[i, j])\n",
    "        samples  = post_samples[i, :, j]\n",
    "\n",
    "        n_less  = np.sum(samples < true_val)\n",
    "        n_equal = np.sum(samples == true_val)   # robust to rare ties\n",
    "        rank    = n_less + rng.uniform(0, 1) * n_equal     # fractional rank\n",
    "        z       = (rank + 1.0) / (S + 1.0)                 # normalized (Uniform(0,1) target)\n",
    "\n",
    "        sbc_records.append({\"dataset\": i, \"parameter\": name, \"rank\": float(rank), \"z\": float(z)})\n",
    "\n",
    "sbc_df = pd.DataFrame(sbc_records)\n",
    "\n",
    "# KL divergence helper (observed || uniform), with Jeffreys smoothing\n",
    "def kl_to_uniform(counts, alpha=0.5, base='e'):\n",
    "    counts = np.asarray(counts, float)\n",
    "    B = len(counts)\n",
    "    p = (counts + alpha) / (counts.sum() + alpha * B)  # smoothed observed probs\n",
    "    q = np.full(B, 1.0 / B)                            # exact uniform probs\n",
    "    kl = np.sum(p * np.log(p / q))\n",
    "    if base == '2':\n",
    "        kl /= np.log(2.0)\n",
    "    return float(max(kl, 0.0))\n",
    "\n",
    "# Histograms with 95% expected bands + diagnostics\n",
    "num_bins = 20\n",
    "bins = np.linspace(0.0, 1.0, num_bins + 1)\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(10, 12), sharey=False)\n",
    "axes = axes.flatten()\n",
    "\n",
    "sbc_summ_rows = []\n",
    "\n",
    "for ax, name in zip(axes, param_names):\n",
    "    sub = sbc_df.loc[sbc_df[\"parameter\"] == name, \"z\"].to_numpy()\n",
    "    counts, _ = np.histogram(sub, bins=bins)\n",
    "    n_sub = len(sub)\n",
    "    exp = n_sub / num_bins\n",
    "\n",
    "    # 95% band for each bin's count under Binomial(n_sub, 1/B)\n",
    "    low, high = binom.interval(0.95, n_sub, 1/num_bins)\n",
    "    low, high = float(low), float(high)\n",
    "\n",
    "    # diagnostics\n",
    "    kl = kl_to_uniform(counts, alpha=0.5, base='e')\n",
    "    chi_stat = float(((counts - exp)**2 / exp).sum())\n",
    "    chi_p = float(chi2.sf(chi_stat, df=num_bins - 1))\n",
    "\n",
    "    sbc_summ_rows.append({\n",
    "        \"parameter\": name, \"n\": n_sub,\n",
    "        \"kl_div_to_uniform\": kl,\n",
    "        \"chi2_stat\": chi_stat, \"chi2_p\": chi_p\n",
    "    })\n",
    "\n",
    "    ax.bar(np.arange(num_bins), counts, width=1, edgecolor='k')\n",
    "    ax.axhline(exp, color='red', linestyle='--', lw=1, label='expected')\n",
    "    ax.axhspan(low, high, color='lightgray', alpha=0.4, zorder=0, label='95% band')\n",
    "    ax.set_title(f\"{name}\\nKL={kl:.3f}  χ²p={chi_p:.3f}\")\n",
    "    ax.set_xlabel(\"rank bin\"); ax.set_ylabel(\"count\")\n",
    "    ax.set_xlim(-0.5, num_bins - 0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "sbc_summary = pd.DataFrame(sbc_summ_rows).sort_values(\"parameter\")\n",
    "display(sbc_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36200046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rt_transform(x, how):\n",
    "    if how in (None, \"none\"): return x.astype(np.float32)\n",
    "    if how == \"log1p\":\n",
    "        if np.any(x < 0): raise ValueError(\"Negative RT encountered with log1p.\")\n",
    "        return np.log1p(x.astype(np.float32))\n",
    "    raise ValueError(f\"Unknown RT transform: {how}\")\n",
    "\n",
    "\n",
    "RT_TRANSFORM = None # Toggling RT log-transform off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c035597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the validation data\n",
    "\n",
    "def load_validation_data_by_subject(base_path):\n",
    "    \"\"\"\n",
    "    Reads\n",
    "      base_path/exp_1/pp1.txt ... pp20.txt,\n",
    "      ...\n",
    "      base_path/exp_14/pp1.txt ... pp20.txt\n",
    "\n",
    "    Returns: list of dicts:\n",
    "      {\"dataset_id\": i, \"participants\": [\n",
    "          {\"subject_id\": \"pp1\",\n",
    "           \"A\": {\"rts\":..., \"stimulus\":..., \"response\":..., \"correct\":...},\n",
    "           \"B\": {...}},\n",
    "          ...\n",
    "      ]}\n",
    "    \"\"\"\n",
    "\n",
    "    datasets = []\n",
    "\n",
    "    for i in range(1, 15):  # exp_1 ... exp_14\n",
    "        exp_path = os.path.join(base_path, f\"exp_{i}\")\n",
    "        participants = []\n",
    "\n",
    "        for j in range(1, 21):  # pp1 ... pp20\n",
    "            df = pd.read_csv(os.path.join(exp_path, f\"pp{j}.txt\"), sep=r\"\\s+\", engine=\"python\")\n",
    "            df.columns = [c.lower() for c in df.columns]\n",
    "\n",
    "            df[\"stimulus\"] = df[\"stim\"].map({\"L\":0,\"R\":1}).astype(\"int64\")\n",
    "            df[\"response\"] = df[\"resp\"].map({\"L\":0,\"R\":1}).astype(\"int64\")\n",
    "            df[\"correct\"]  = df[\"correct\"].astype(\"int64\")\n",
    "            rt_raw = df[\"rt\"].astype(\"float32\").to_numpy()\n",
    "            rt_for_model = apply_rt_transform(rt_raw, RT_TRANSFORM)\n",
    "\n",
    "            subj = {\"subject_id\": f\"pp{j}\"}\n",
    "            for cond in [\"A\",\"B\"]:\n",
    "                idx = df.index[df[\"cond\"]==cond].to_numpy()\n",
    "                subj[cond] = {\n",
    "                    \"rts\":      rt_for_model[idx],\n",
    "                    \"stimulus\": df.loc[idx,\"stimulus\"].to_numpy(np.int64),\n",
    "                    \"response\": df.loc[idx,\"response\"].to_numpy(np.int64),\n",
    "                    \"correct\":  df.loc[idx,\"correct\"].to_numpy(np.int64),\n",
    "                }\n",
    "\n",
    "            participants.append(subj)\n",
    "        datasets.append({\"dataset_id\": i, \"participants\": participants})\n",
    "    return datasets\n",
    "\n",
    "base = r\"C:/Users/emils/Documents/uni/M_Thesis/diffusion-bayesflow/data/real/validation_text_data/validation_text_data\"\n",
    "val_data_by_subj = load_validation_data_by_subject(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd02e29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to compute summary stats from raw data\n",
    "\n",
    "def _cond_stim_stats(rts_raw, correct, stimulus, min_trials=3):\n",
    "    \"\"\"\n",
    "    Compute (mrt, vrt, pe) per stimulus (0='L', 1='R') from RAW RTs.\n",
    "    Returns dict with mrtL, vrtL, peL, mrtR, vrtR, peR.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for stim_val, label in ((0, \"L\"), (1, \"R\")):\n",
    "        mask = (stimulus == stim_val)\n",
    "        n = int(mask.sum())\n",
    "        if n < min_trials:\n",
    "            raise ValueError(f\"Too few trials for stimulus {label}: n={n}\")\n",
    "        r = np.asarray(rts_raw[mask], dtype=np.float64)\n",
    "        c = np.asarray(correct[mask], dtype=np.float64)\n",
    "        if not (np.isfinite(r).all() and np.isfinite(c).all()):\n",
    "            raise ValueError(f\"Non-finite values for stimulus {label}\")\n",
    "        out[f\"mrt{label}\"] = float(np.mean(r))\n",
    "        out[f\"vrt{label}\"] = float(np.var(r, ddof=1))\n",
    "        out[f\"pe{label}\"]  = float(1.0 - np.mean(c))\n",
    "    return out\n",
    "\n",
    "# Obtaining posteriors for each subject per condition\n",
    "\n",
    "def run_inference_per_subject_ezbf(\n",
    "    wf, datasets, num_samples=500, min_trials=3, return_stats=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Per-subject EZ-BF inference (by condition), assuming RTs are already RAW (no transforms).\n",
    "    Input `datasets` format (like val_data_by_subj):\n",
    "      [{\"dataset_id\": i,\n",
    "        \"participants\": [{\"subject_id\": \"pp1\",\n",
    "                          \"A\": {\"rts\":..., \"stimulus\":..., \"response\":..., \"correct\":...},\n",
    "                          \"B\": {...}}, ...]}]\n",
    "    Uses: stimulus 0='L', 1='R'; correct 0/1.\n",
    "    Returns:\n",
    "      [{\"dataset_id\": i,\n",
    "        \"subjects\": [{\"subject_id\": sid,\n",
    "                      \"A_samples\": (num_samples,6) or None,\n",
    "                      \"B_samples\": (num_samples,6) or None,\n",
    "                      optional A_stats/B_stats if return_stats=True}, ...]}]\n",
    "    \"\"\"\n",
    "    out = []\n",
    "\n",
    "    for ds in datasets:\n",
    "        dsid = ds[\"dataset_id\"]\n",
    "        subj_results = []\n",
    "\n",
    "        for subj in ds[\"participants\"]:\n",
    "            sid = subj[\"subject_id\"]\n",
    "            rec = {\"subject_id\": sid}\n",
    "\n",
    "            for cond in (\"A\", \"B\"):\n",
    "                cd = subj[cond]\n",
    "                rts_raw  = np.asarray(cd[\"rts\"], dtype=np.float32)\n",
    "                stimulus = np.asarray(cd[\"stimulus\"], dtype=np.int64)\n",
    "                correct  = np.asarray(cd[\"correct\"],  dtype=np.float32)\n",
    "\n",
    "                if rts_raw.size == 0 or stimulus.size == 0 or correct.size == 0:\n",
    "                    rec[f\"{cond}_samples\"] = None\n",
    "                    rec[f\"{cond}_error\"] = \"no trials\"\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    # per-subject, per-condition moments\n",
    "                    stats = _cond_stim_stats(rts_raw, correct, stimulus, min_trials=min_trials)\n",
    "\n",
    "                    \n",
    "                    conditions = {\n",
    "                        f\"mrt{s}\": np.array([[stats[f\"mrt{s}\"]]], dtype=np.float32) for s in (\"L\",\"R\")\n",
    "                    } | {\n",
    "                        f\"vrt{s}\": np.array([[stats[f\"vrt{s}\"]]], dtype=np.float32) for s in (\"L\",\"R\")\n",
    "                    } | {\n",
    "                        f\"pe{s}\":  np.array([[stats[f\"pe{s}\"] ]], dtype=np.float32) for s in (\"L\",\"R\")\n",
    "                    }\n",
    "\n",
    "                    samples = wf.sample(conditions=conditions, num_samples=num_samples, to_numpy=True)\n",
    "\n",
    "                    arr = np.column_stack([\n",
    "                        samples['vL'][0].squeeze(),\n",
    "                        samples['vR'][0].squeeze(),\n",
    "                        samples['a'][0].squeeze(),\n",
    "                        samples['z'][0].squeeze(),\n",
    "                        samples['terL'][0].squeeze(),\n",
    "                        samples['terR'][0].squeeze(),\n",
    "                    ]).astype(np.float32)\n",
    "\n",
    "                    rec[f\"{cond}_samples\"] = arr\n",
    "                    if return_stats:\n",
    "                        rec[f\"{cond}_stats\"] = stats\n",
    "\n",
    "                except Exception as e:\n",
    "                    rec[f\"{cond}_samples\"] = None\n",
    "                    rec[f\"{cond}_error\"] = str(e)\n",
    "\n",
    "            subj_results.append(rec)\n",
    "\n",
    "        out.append({\"dataset_id\": dsid, \"subjects\": subj_results})\n",
    "\n",
    "    return out\n",
    "\n",
    "post_by_subj = run_inference_per_subject_ezbf(wf, val_data_by_subj, num_samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690089ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing condition differences by parameter, on subject-level\n",
    "\n",
    "def make_subject_deltas(post_by_subj):\n",
    "    \"\"\"\n",
    "    Returns a flat list of rows:\n",
    "      {\"dataset_id\": i, \"subject_id\": sid,\n",
    "       \"delta\": {\"v\": (K,), \"a\": (K,), \"z\": (K,), \"ter\": (K,)} }\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for ds in post_by_subj:\n",
    "        dsid = ds[\"dataset_id\"]\n",
    "        for s in ds[\"subjects\"]:\n",
    "            A, B = s.get(\"A_samples\"), s.get(\"B_samples\")\n",
    "            if A is None or B is None:\n",
    "                continue\n",
    "\n",
    "            vA, vB   = (A[:,0] + A[:,1]), (B[:,0] + B[:,1])\n",
    "            aA, aB   = A[:,2], B[:,2]\n",
    "            zA, zB   = np.log(1 - A[:,3]), np.log(1 - B[:,3])\n",
    "            terA, terB = (A[:,4] + A[:,5]), (B[:,4] + B[:,5])\n",
    "\n",
    "            rows.append({\n",
    "                \"dataset_id\": dsid,\n",
    "                \"subject_id\": s[\"subject_id\"],\n",
    "                \"delta\": {\"v\": vB - vA, \"a\": aB - aA, \"z\": zB - zA, \"ter\": terB - terA},\n",
    "            })\n",
    "    return rows\n",
    "\n",
    "subj_deltas = make_subject_deltas(post_by_subj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0589e181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining group-level condition differences\n",
    "\n",
    "def aggregate_group_mean(subj_deltas, num_draws=4000, seed=123):\n",
    "    \"\"\"\n",
    "    Composition sampling:\n",
    "      For each dataset & parameter:\n",
    "        draw one index from each subject's Δ-samples, average -> one μ draw.\n",
    "    Returns: [{\"dataset_id\": i, \"v_samples_group\": (M,), \"a_samples_group\":..., ...}, ...]\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # collect per dataset/param\n",
    "    grouped = {}\n",
    "    for row in subj_deltas:\n",
    "        dsid = row[\"dataset_id\"]\n",
    "        grouped.setdefault(dsid, {p: [] for p in [\"v\",\"a\",\"z\",\"ter\"]})\n",
    "        for p in [\"v\",\"a\",\"z\",\"ter\"]:\n",
    "            grouped[dsid][p].append(row[\"delta\"][p])\n",
    "\n",
    "    group_samples = []\n",
    "    for dsid, by_param in grouped.items():\n",
    "        rec = {\"dataset_id\": dsid}\n",
    "        for p, arrs in by_param.items():\n",
    "            if len(arrs) == 0:\n",
    "                rec[f\"{p}_samples_group\"] = None\n",
    "                continue\n",
    "            K = arrs[0].shape[0]\n",
    "            S = len(arrs)\n",
    "            stacked = np.stack(arrs, axis=1)\n",
    "\n",
    "            # indices: for each group draw, choose a (possibly different) sample per subject\n",
    "            idx = rng.integers(0, K, size=(num_draws, S))\n",
    "            # vectorized gather; average across subjects\n",
    "            draws = stacked[idx, np.arange(S)].mean(axis=1)\n",
    "            rec[f\"{p}_samples_group\"] = draws.astype(np.float32)\n",
    "        group_samples.append(rec)\n",
    "    return group_samples\n",
    "\n",
    "group_samples = aggregate_group_mean(subj_deltas, num_draws=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c53b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing HDIs\n",
    "\n",
    "def compute_hdi_from_group(group_samples, hdi_prob=0.99):\n",
    "    rows = []\n",
    "    for ds in group_samples:\n",
    "        dsid = ds[\"dataset_id\"]\n",
    "        rec = {\"dataset_id\": dsid}\n",
    "        for p in [\"v\",\"a\",\"z\",\"ter\"]:\n",
    "            s = ds.get(f\"{p}_samples_group\")\n",
    "            if s is None or len(s)==0:\n",
    "                rec.update({f\"{p}_diff_mean\": np.nan,\n",
    "                            f\"{p}_diff_median\": np.nan,\n",
    "                            f\"{p}_hdi_lower\": np.nan,\n",
    "                            f\"{p}_hdi_upper\": np.nan,\n",
    "                            f\"{p}_excludes_zero\": False})\n",
    "                continue\n",
    "            lo = np.percentile(s, (1-hdi_prob)/2*100)\n",
    "            hi = np.percentile(s, (1+hdi_prob)/2*100)\n",
    "            rec.update({\n",
    "                f\"{p}_diff_mean\": float(np.mean(s)),\n",
    "                f\"{p}_diff_median\": float(np.median(s)),\n",
    "                f\"{p}_hdi_lower\": float(lo),\n",
    "                f\"{p}_hdi_upper\": float(hi),\n",
    "                f\"{p}_excludes_zero\": not (lo <= 0 <= hi),\n",
    "            })\n",
    "        rows.append(rec)\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fb8688",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdi_df = compute_hdi_from_group(group_samples, hdi_prob=0.99)\n",
    "hdi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0a7f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "fig, axes = plt.subplots(3, 2, figsize=(14, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "param_names = [\"v\", \"a\", \"z\", \"ter\"]\n",
    "\n",
    "for i, param in enumerate(param_names):\n",
    "    ax = axes[i]\n",
    "        \n",
    "    subset = hdi_df[[\"dataset_id\", \n",
    "                    f\"{param}_diff_mean\", \n",
    "                    f\"{param}_hdi_lower\", \n",
    "                    f\"{param}_hdi_upper\", \n",
    "                    f\"{param}_excludes_zero\"]].copy()\n",
    "\n",
    "    subset = subset.sort_values(\"dataset_id\")\n",
    "\n",
    "    for _, row in subset.iterrows():\n",
    "        y = row[\"dataset_id\"]\n",
    "        color = \"tab:blue\" if row[f\"{param}_excludes_zero\"] else \"gray\"\n",
    "        ax.plot([row[f\"{param}_hdi_lower\"], row[f\"{param}_hdi_upper\"]], [y, y], color=color)\n",
    "        ax.plot(row[f\"{param}_diff_mean\"], y, \"o\", color=color)\n",
    "\n",
    "    ax.axvline(0, color=\"black\", linestyle=\"--\")\n",
    "    ax.set_xlabel(\"ΔB-A\")\n",
    "    ax.set_ylabel(\"Dataset\")\n",
    "    ax.set_title(f\"Δ{param}\")\n",
    "    ax.set_yticks(range(1, 15))\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "for j in range(len(param_names), 6):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12f773c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing BFs\n",
    "\n",
    "PRIOR_BOUNDS = {\n",
    "    \"vL\":   (0.0, 3.0),\n",
    "    \"vR\":   (0.0, 3.0),\n",
    "    \"a\":    (0.5, 2.5),\n",
    "    \"z\":    (0.05, 0.95),\n",
    "    \"terL\": (0.2, 0.6),\n",
    "    \"terR\": (0.2, 0.6),\n",
    "}\n",
    "\n",
    "def subject_counts(subj_deltas):\n",
    "    S = {}\n",
    "    for row in subj_deltas:\n",
    "        S[row[\"dataset_id\"]] = S.get(row[\"dataset_id\"], 0) + 1\n",
    "    return S\n",
    "\n",
    "def moment_matched_prior_sd_for_mu(S, num_draws=20000, seed=123, bounds=PRIOR_BOUNDS):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    def U(name): lo, hi = bounds[name]; return rng.uniform(lo, hi, size=(num_draws, S))\n",
    "    vL_A, vR_A = U(\"vL\"), U(\"vR\"); vL_B, vR_B = U(\"vL\"), U(\"vR\")\n",
    "    a_A, a_B   = U(\"a\"), U(\"a\")\n",
    "    z_A, z_B   = U(\"z\"), U(\"z\")\n",
    "    tL_A, tR_A = U(\"terL\"), U(\"terR\"); tL_B, tR_B = U(\"terL\"), U(\"terR\")\n",
    "    vA, vB   = vL_A+vR_A, vL_B+vR_B\n",
    "    aA, aB   = a_A, a_B\n",
    "    zA, zB   = np.log(1 - z_A), np.log(1 - z_B)\n",
    "    terA,terB= tL_A+tR_A,      tL_B+tR_B\n",
    "    dV, dA, dZ, dT = (vB-vA), (aB-aA), (zB-zA), (terB-terA)\n",
    "    muV, muA, muZ, muT = dV.mean(1), dA.mean(1), dZ.mean(1), dT.mean(1)\n",
    "    return {\"v\": float(muV.std(ddof=1)),\n",
    "            \"a\": float(muA.std(ddof=1)),\n",
    "            \"z\": float(muZ.std(ddof=1)),\n",
    "            \"ter\": float(muT.std(ddof=1))}\n",
    "\n",
    "def build_model_prior_sd_map(subj_deltas, num_draws=20000, seed=123, bounds=PRIOR_BOUNDS):\n",
    "    S_map = subject_counts(subj_deltas)\n",
    "    return {dsid: moment_matched_prior_sd_for_mu(S, num_draws=num_draws, seed=seed, bounds=bounds)\n",
    "            for dsid, S in S_map.items()}\n",
    "\n",
    "\n",
    "from scipy.stats import norm, gaussian_kde\n",
    "\n",
    "def compute_bf_from_group(group_samples, prior_sd=1.0, bw_mult=1.2, floor=1e-300):\n",
    "    \"\"\"\n",
    "    Simple Savage-Dickey BF at μ=0 using Normal(0, prior_sd) and KDE for posterior.\n",
    "    prior_sd can be scalar, per-param dict, or {dsid: {param: sd}}.\n",
    "    Returns BF10, log BF10.\n",
    "    \"\"\"\n",
    "    def _sd_for(dsid, p):\n",
    "        if isinstance(prior_sd, dict):\n",
    "            if dsid in prior_sd:\n",
    "                return float(prior_sd[dsid][p])\n",
    "            if p in prior_sd:\n",
    "                return float(prior_sd[p])\n",
    "        return float(prior_sd)\n",
    "\n",
    "    rows, tiny = [], np.finfo(float).tiny\n",
    "    for ds in group_samples:\n",
    "        dsid = ds[\"dataset_id\"]; rec = {\"dataset_id\": dsid}\n",
    "        for p in [\"v\",\"a\",\"z\",\"ter\"]:\n",
    "            s = ds.get(f\"{p}_samples_group\")\n",
    "            if s is None or len(s)==0:\n",
    "                rec.update({f\"{p}_bf_10\": np.nan, f\"{p}_ln_bf_10\": np.nan, f\"{p}_log10_bf_10\": np.nan})\n",
    "                continue\n",
    "\n",
    "            s = np.asarray(s, np.float64)\n",
    "            kde = gaussian_kde(s)\n",
    "            try: kde.set_bandwidth(kde.factor * bw_mult)\n",
    "            except Exception: pass\n",
    "            post0 = max(float(kde.evaluate(0.0)[0]), floor)\n",
    "\n",
    "            sd = max(_sd_for(dsid, p), tiny)\n",
    "            prior0 = float(norm(0, sd).pdf(0.0))\n",
    "\n",
    "            log_bf  = np.log(prior0) - np.log(post0)\n",
    "            bf10   = np.exp(log_bf) if log_bf < 700 else np.inf\n",
    "\n",
    "            rec[f\"{p}_bf_10\"]       = float(bf10)\n",
    "            rec[f\"{p}_log_bf_10\"]   = float(log_bf)\n",
    "            rec[f\"{p}_prior_sd\"]    = sd\n",
    "        rows.append(rec)\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de0dbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model-based σ_μ map from training priors (depends on S per dataset)\n",
    "model_sd_map = build_model_prior_sd_map(subj_deltas, num_draws=20000, seed=123)\n",
    "\n",
    "bf_df = compute_bf_from_group(group_samples, prior_sd=model_sd_map,\n",
    "                                     bw_mult=1.0, floor=1e-300)\n",
    "\n",
    "bf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539b2d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codebook of experimental manipulations based on Dutilh et al. (2019)\n",
    "# The codebook was introduced after the data inference pipeline and\n",
    "# decision rules were fixed\n",
    "\n",
    "ground_truth_planned_analysis = [\n",
    "    {\"dataset_id\": 1, \"manipulated\": {}},\n",
    "    {\"dataset_id\": 2, \"manipulated\": {\"v\": \"B\"}},\n",
    "    {\"dataset_id\": 3, \"manipulated\": {\"a\": \"B\"}},\n",
    "    {\"dataset_id\": 4, \"manipulated\": {\"z\": \"B\"}},\n",
    "    {\"dataset_id\": 5, \"manipulated\": {\"v\": \"B\", \"a\": \"B\"}},\n",
    "    {\"dataset_id\": 6, \"manipulated\": {\"v\": \"B\", \"z\": \"B\"}},\n",
    "    {\"dataset_id\": 7, \"manipulated\": {\"a\": \"B\", \"z\": \"B\"}},\n",
    "    {\"dataset_id\": 8, \"manipulated\": {\"v\": \"A\", \"a\": \"B\"}},\n",
    "    {\"dataset_id\": 9, \"manipulated\": {\"v\": \"A\", \"z\": \"B\"}},\n",
    "    {\"dataset_id\": 10, \"manipulated\": {\"a\": \"A\", \"z\": \"B\"}},\n",
    "    {\"dataset_id\": 11, \"manipulated\": {\"v\": \"A\", \"a\": \"B\", \"z\": \"B\"}},\n",
    "    {\"dataset_id\": 12, \"manipulated\": {\"v\": \"B\", \"a\": \"A\", \"z\": \"B\"}},\n",
    "    {\"dataset_id\": 13, \"manipulated\": {\"v\": \"B\", \"a\": \"B\", \"z\": \"A\"}},\n",
    "    {\"dataset_id\": 14, \"manipulated\": {\"v\": \"B\", \"a\": \"B\", \"z\": \"B\"}},\n",
    "]\n",
    "\n",
    "ground_truth_alt_analysis_1 = [\n",
    "    {'dataset_id': 1, 'manipulated': {}},\n",
    "    {'dataset_id': 2, 'manipulated': {'v': 'B'}},\n",
    "    {'dataset_id': 3, 'manipulated': {'a': 'B', 'v': 'B'}},\n",
    "    {'dataset_id': 4, 'manipulated': {'z': 'B'}},\n",
    "    {'dataset_id': 5, 'manipulated': {'v': 'B', 'a': 'B'}},\n",
    "    {'dataset_id': 6, 'manipulated': {'v': 'B', 'z': 'B'}},\n",
    "    {'dataset_id': 7, 'manipulated': {'a': 'B', 'z': 'B', 'v': 'B'}},\n",
    "    {'dataset_id': 9, 'manipulated': {'v': 'A', 'z': 'B'}},\n",
    "    {'dataset_id': 10, 'manipulated': {'a': 'A', 'z': 'B', 'v': 'A'}},\n",
    "    {'dataset_id': 13, 'manipulated': {'v': 'B', 'a': 'B', 'z': 'A'}},\n",
    "    {'dataset_id': 14, 'manipulated': {'v': 'B', 'a': 'B', 'z': 'B'}}\n",
    "]\n",
    "\n",
    "ground_truth_alt_analysis_2 = [\n",
    "    {'dataset_id': 1, 'manipulated': {}},\n",
    "    {'dataset_id': 2, 'manipulated': {'v': 'B'}},\n",
    "    {'dataset_id': 3, 'manipulated': {'a': 'B', 'ter': 'B'}},\n",
    "    {'dataset_id': 4, 'manipulated': {'z': 'B'}},\n",
    "    {'dataset_id': 5, 'manipulated': {'v': 'B', 'a': 'B', 'ter': 'B'}},\n",
    "    {'dataset_id': 6, 'manipulated': {'v': 'B', 'z': 'B'}},\n",
    "    {'dataset_id': 7, 'manipulated': {'a': 'B', 'z': 'B', 'ter': 'B'}},\n",
    "    {'dataset_id': 8, 'manipulated': {'v': 'A', 'a': 'B', 'ter': 'B'}},\n",
    "    {'dataset_id': 9, 'manipulated': {'v': 'A', 'z': 'B'}},\n",
    "    {'dataset_id': 10, 'manipulated': {'a': 'A', 'z': 'B', 'ter': 'A'}},\n",
    "    {'dataset_id': 11, 'manipulated': {'v': 'A', 'a': 'B', 'z': 'B', 'ter': 'B'}},\n",
    "    {'dataset_id': 12, 'manipulated': {'v': 'B', 'a': 'A', 'z': 'B', 'ter': 'A'}},\n",
    "    {'dataset_id': 13, 'manipulated': {'v': 'B', 'a': 'B', 'z': 'A', 'ter': 'B'}},\n",
    "    {'dataset_id': 14, 'manipulated': {'v': 'B', 'a': 'B', 'z': 'B', 'ter': 'B'}}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70c123b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to automatically determine per parameter and per dataset inference\n",
    "# on the basis of the HDI and BF decision rules and without human judgment\n",
    "\n",
    "def infer_direction(hdi_df, bf_df, bf_threshold=3.0, params=(\"v\",\"a\",\"z\",\"ter\")):\n",
    "    \"\"\"\n",
    "    Calls a condition difference when:\n",
    "      - 99% HDI excludes 0, AND\n",
    "      - BF10 > bf_threshold.\n",
    "    Direction is by the sign of the median Δ = (B - A):\n",
    "      Δ > 0 -> \"B\", Δ < 0 -> \"A\", Δ = 0 -> \"0\".\n",
    "    Calls \"0\" (no difference) when:\n",
    "      - HDI includes 0 AND BF10 < 1/bf_threshold.\n",
    "    Otherwise, \"0\" (inconclusive).\n",
    "    \"\"\"\n",
    "    merged = pd.merge(hdi_df, bf_df, on=\"dataset_id\", suffixes=(\"_hdi\", \"_bf\"))\n",
    "    rows = []\n",
    "    inv_thr = 1.0 / bf_threshold\n",
    "\n",
    "    for _, r in merged.iterrows():\n",
    "        dsid = r[\"dataset_id\"]\n",
    "        for p in params:\n",
    "            hdi_excl = bool(r.get(f\"{p}_excludes_zero\"))\n",
    "            bf = r.get(f\"{p}_bf_10\")\n",
    "            med = r.get(f\"{p}_diff_median\")\n",
    "\n",
    "            if hdi_excl and (bf is not None) and (bf > bf_threshold):\n",
    "                inferred = \"B\" if med > 0 else (\"A\" if med < 0 else \"0\")\n",
    "            elif (not hdi_excl) and (bf is not None) and (bf < inv_thr):\n",
    "                inferred = \"0\"\n",
    "            else:\n",
    "                inferred = \"0\"\n",
    "\n",
    "            rows.append({\n",
    "                \"dataset_id\": dsid,\n",
    "                \"param\": p,\n",
    "                \"hdi_excludes_zero\": hdi_excl,\n",
    "                \"hdi_B-A_median\": med,\n",
    "                \"bf_10\": bf,\n",
    "                \"inferred_direction\": inferred,\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfddf2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferred_df = infer_direction(hdi_df, bf_df)\n",
    "\n",
    "inferred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7c741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically scoring the inferences against the codebook without human judgment\n",
    "\n",
    "def inference_evaluation(inference_df, ground_truth):\n",
    "    \"\"\"\n",
    "    Returns rates per parameter (and a Total row) with only:\n",
    "      - Correct  (correct directional + correct null)\n",
    "      - Miss     (called 0 but truth A/B)\n",
    "      - False Alarm (called A/B but truth 0)\n",
    "      - Flip     (called A vs truth B or vice versa)\n",
    "    \"\"\"\n",
    "    counts = defaultdict(lambda: {\"correct_dir\": 0, \"correct_null\": 0,\n",
    "                                  \"miss\": 0, \"false_alarm\": 0, \"flip\": 0, \"total\": 0})\n",
    "\n",
    "    # Lookup predictions\n",
    "    pred = {(r[\"dataset_id\"], r[\"param\"]): r[\"inferred_direction\"]\n",
    "            for _, r in inference_df.iterrows()}\n",
    "\n",
    "    for gt in ground_truth:\n",
    "        ds = gt[\"dataset_id\"]\n",
    "        truth_map = gt.get(\"manipulated\", {})\n",
    "        for p in [\"v\", \"a\", \"z\", \"ter\"]:\n",
    "            truth = truth_map.get(p, \"0\")\n",
    "            guess = pred.get((ds, p))\n",
    "            if guess is None:\n",
    "                continue\n",
    "\n",
    "            if truth == \"0\" and guess == \"0\":\n",
    "                counts[p][\"correct_null\"] += 1\n",
    "            elif truth in (\"A\",\"B\") and guess == truth:\n",
    "                counts[p][\"correct_dir\"] += 1\n",
    "            elif truth in (\"A\",\"B\") and guess == \"0\":\n",
    "                counts[p][\"miss\"] += 1\n",
    "            elif truth == \"0\" and guess in (\"A\",\"B\"):\n",
    "                counts[p][\"false_alarm\"] += 1\n",
    "            elif truth in (\"A\",\"B\") and guess in (\"A\",\"B\") and guess != truth:\n",
    "                counts[p][\"flip\"] += 1\n",
    "\n",
    "            counts[p][\"total\"] += 1\n",
    "\n",
    "    # Per-parameter rates\n",
    "    rows = []\n",
    "    for p, c in counts.items():\n",
    "        T = c[\"total\"]\n",
    "        if T == 0:\n",
    "            rows.append({\"parameter\": p,\n",
    "                         \"Correct\": np.nan, \"Miss\": np.nan,\n",
    "                         \"False Alarm\": np.nan, \"Flip\": np.nan})\n",
    "            continue\n",
    "        correct = (c[\"correct_dir\"] + c[\"correct_null\"]) / T\n",
    "        rows.append({\"parameter\": p,\n",
    "                     \"Correct\": correct,\n",
    "                     \"Miss\": c[\"miss\"] / T,\n",
    "                     \"False Alarm\": c[\"false_alarm\"] / T,\n",
    "                     \"Flip\": c[\"flip\"] / T})\n",
    "\n",
    "    # Total row\n",
    "    tot = {\"correct_dir\": 0, \"correct_null\": 0, \"miss\": 0, \"false_alarm\": 0, \"flip\": 0, \"total\": 0}\n",
    "    for c in counts.values():\n",
    "        for k in tot: tot[k] += c[k]\n",
    "    TT = tot[\"total\"]\n",
    "    total_row = {\"parameter\": \"Total\"}\n",
    "    if TT == 0:\n",
    "        total_row.update({\"Correct\": np.nan, \"Miss\": np.nan, \"False Alarm\": np.nan, \"Flip\": np.nan})\n",
    "    else:\n",
    "        total_row.update({\n",
    "            \"Correct\": (tot[\"correct_dir\"] + tot[\"correct_null\"]) / TT,\n",
    "            \"Miss\": tot[\"miss\"] / TT,\n",
    "            \"False Alarm\": tot[\"false_alarm\"] / TT,\n",
    "            \"Flip\": tot[\"flip\"] / TT\n",
    "        })\n",
    "\n",
    "    return pd.concat([pd.DataFrame(rows), pd.DataFrame([total_row])], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d35529",
   "metadata": {},
   "outputs": [],
   "source": [
    "planned_eval = inference_evaluation(inferred_df, ground_truth_planned_analysis)\n",
    "planned_eval.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe44a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_eval_1 = inference_evaluation(inferred_df, ground_truth_alt_analysis_1)\n",
    "alt_eval_1.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bc7953",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_eval_2 = inference_evaluation(inferred_df, ground_truth_alt_analysis_2)\n",
    "alt_eval_2.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91936dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out non-decision time inferences\n",
    "inferred_df_no_ter = inferred_df[inferred_df[\"param\"] != \"ter\"].copy()\n",
    "\n",
    "# Evaluate with non-decision time excluded\n",
    "eval_no_ter = inference_evaluation(inferred_df_no_ter, ground_truth_planned_analysis)\n",
    "eval_no_ter.round(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
