{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da725003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import bayesflow as bf\n",
    "import pickle\n",
    "import EZ2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff23b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def _p_bottom_safe(nu, z, a, s=0.1):\n",
    "    \"\"\"Bottom-hit probability with a fallback for near-zero nu.\"\"\"\n",
    "    if abs(nu) < 1e-12:\n",
    "        return (a - z) / a\n",
    "    s2 = s * s\n",
    "    num = np.exp(-2*a*nu/s2) - np.exp(-2*z*nu/s2)\n",
    "    den = np.exp(-2*a*nu/s2) - 1.0\n",
    "    return float(np.clip(num / den, 0.0, 1.0))\n",
    "\n",
    "def _safe_rddexit(size, nu, z, a, top_boundary):\n",
    "    \"\"\"Call EZ2.rddexit and always return a list (even for size==1).\"\"\"\n",
    "    if size <= 0:\n",
    "        return []\n",
    "    arr = EZ2.rddexit(size, nu, z, a, top_boundary=top_boundary)\n",
    "    if np.isscalar(arr):\n",
    "        return [float(arr)]\n",
    "    return [float(x) for x in np.asarray(arr).ravel()]\n",
    "\n",
    "def _sample_times(size, nu, z, a, s=0.1, rng=None):\n",
    "    \"\"\"Safer equivalent of rddexitj using robust fallbacks.\"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "    p0 = _p_bottom_safe(nu, z, a, s=s)\n",
    "    n_bottom = rng.binomial(size, p0)\n",
    "    n_top = size - n_bottom\n",
    "    et_bottom = _safe_rddexit(n_bottom, nu, z, a, top_boundary=False)\n",
    "    et_top    = _safe_rddexit(n_top,    nu, z, a, top_boundary=True)\n",
    "    return et_bottom, et_top\n",
    "\n",
    "# Forward model\n",
    "\n",
    "def forward_model_ez2(\n",
    "    vL, vR, a, z, terL, terR, n_trials=200, rng=None,\n",
    "    rt_transform=\"log1p\" # \"log1p\" or \"none\"\n",
    "):\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    # scale to s=0.1 used in functions EZ2\n",
    "    c = 0.1\n",
    "    vL_ez, vR_ez = float(vL)*c, float(vR)*c\n",
    "    a_ez = float(a)*c\n",
    "\n",
    "    # converting relative z to absolute z\n",
    "    z_abs = float(z) * a_ez\n",
    "    eps = 1e-9 * a_ez\n",
    "    z_abs = min(max(z_abs, eps), a_ez - eps)\n",
    "\n",
    "    nA = n_trials // 2\n",
    "    nB = n_trials - nA\n",
    "\n",
    "    # A condition (Left correct): top->Left(0), bottom->Right(1)\n",
    "    et_b_A, et_t_A = _sample_times(nA, vL_ez, z_abs, a_ez, s=0.1, rng=rng)\n",
    "    et_b_A = np.asarray(et_b_A, dtype=np.float64)\n",
    "    et_t_A = np.asarray(et_t_A, dtype=np.float64)\n",
    "    nAb, nAt = et_b_A.size, et_t_A.size\n",
    "    dts_A = np.empty(nA, dtype=np.float64); dts_A[:nAb] = et_b_A; dts_A[nAb:] = et_t_A\n",
    "    choices_A = np.empty(nA, dtype=np.int64); choices_A[:nAb] = 1; choices_A[nAb:] = 0\n",
    "    correct_A = np.empty(nA, dtype=np.int64); correct_A[:nAb] = 0; correct_A[nAb:] = 1\n",
    "    stim_A = np.zeros(nA, dtype=np.int64)\n",
    "\n",
    "    # B condition (Right correct): top->Right(1), bottom->Left(0)\n",
    "    et_b_B, et_t_B = _sample_times(nB, vR_ez, a_ez - z_abs, a_ez, s=0.1, rng=rng)\n",
    "    et_b_B = np.asarray(et_b_B, dtype=np.float64)\n",
    "    et_t_B = np.asarray(et_t_B, dtype=np.float64)\n",
    "    nBb, nBt = et_b_B.size, et_t_B.size\n",
    "    dts_B = np.empty(nB, dtype=np.float64); dts_B[:nBb] = et_b_B; dts_B[nBb:] = et_t_B\n",
    "    choices_B = np.empty(nB, dtype=np.int64); choices_B[:nBb] = 0; choices_B[nBb:] = 1\n",
    "    correct_B = np.empty(nB, dtype=np.int64); correct_B[:nBb] = 0; correct_B[nBb:] = 1\n",
    "    stim_B = np.ones(nB, dtype=np.int64)\n",
    "\n",
    "    dts = np.concatenate([dts_A, dts_B])\n",
    "    choices = np.concatenate([choices_A, choices_B])\n",
    "    correct = np.concatenate([correct_A, correct_B])\n",
    "    stimulus = np.concatenate([stim_A, stim_B])\n",
    "\n",
    "    perm = rng.permutation(n_trials)\n",
    "    dts, choices, correct, stimulus = dts[perm], choices[perm], correct[perm], stimulus[perm]\n",
    "\n",
    "    # Add ter and optional log-transform\n",
    "    rts = dts + np.where(choices == 0, terL, terR)\n",
    "    if rt_transform == \"log1p\":\n",
    "        rts = np.log1p(rts)\n",
    "\n",
    "    return {\n",
    "        \"rts\": rts.astype(np.float32),\n",
    "        \"choices\": choices,\n",
    "        \"stimulus\": stimulus,\n",
    "        \"correct\": correct,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592c6550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior():\n",
    "  params = {}\n",
    "\n",
    "  # Drift rates v toward left and right responses\n",
    "  params['vL'] = np.random.uniform(0.1, 6.0)\n",
    "  params['vR'] = np.random.uniform(0.1, 6.0)\n",
    "\n",
    "  # Boundary separation a\n",
    "  params['a'] = np.random.uniform(0.3, 4.0)\n",
    "\n",
    "  # Relative starting point z\n",
    "  params['z'] = np.random.uniform(0.1, 0.9)\n",
    "\n",
    "  # Non-decision times ter for left and right responses (in seconds)\n",
    "  params['terL'] = np.random.uniform(0.1, 1.0)\n",
    "  params['terR'] = np.random.uniform(0.1, 1.0)\n",
    "\n",
    "  return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247f4a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator = bf.make_simulator([prior, forward_model_ez2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa57bd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_names = ['vL', 'vR', 'a', 'z', 'terL', 'terR']\n",
    "data_names = ['rts', 'stimulus', 'choices']  # Removed 'correct' for redundancy of information\n",
    "# 'correct' can be derived from 'choices' and 'stimulus' so we it's not needed\n",
    "\n",
    "adapter = (\n",
    "    bf.adapters.Adapter()\n",
    "    .keep(param_names + data_names)\n",
    "    .to_array()\n",
    "    .convert_dtype(\"float64\", \"float32\")\n",
    "    .expand_dims(\"rts\", axis=-1)\n",
    "    .expand_dims(\"choices\", axis=-1)\n",
    "    .expand_dims(\"stimulus\", axis=-1)\n",
    "    .concatenate(param_names, into=\"inference_variables\")\n",
    "    .concatenate(data_names, into=\"summary_variables\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e28dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayesflow.networks import CouplingFlow, DeepSet\n",
    "from bayesflow.workflows import BasicWorkflow\n",
    "\n",
    "summary_net = DeepSet(\n",
    "    summary_dim=16,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "flow = CouplingFlow(\n",
    "    num_coupling_layers=6,\n",
    "    hidden_units=[128, 128],\n",
    "    coupling_type=\"spline\",\n",
    "    batch_norm=True,\n",
    "    dropout=0.05,\n",
    "    tail_bound=6.0\n",
    ")\n",
    "\n",
    "wf = BasicWorkflow(\n",
    "    simulator=simulator,\n",
    "    adapter=adapter,\n",
    "    summary_network=summary_net,\n",
    "    inference_network=flow,\n",
    "    inference_variables=[\"inference_variables\"],\n",
    "    summary_variables=[\"summary_variables\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79147629",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_approx = keras.models.load_model(\n",
    "  \"C:/Users/emils/Documents/uni/M_Thesis/diffusion-bayesflow/scripts/standard_model.keras\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d14d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wf = BasicWorkflow(\n",
    "    simulator=simulator,\n",
    "    adapter=adapter,\n",
    "    summary_network=summary_net,\n",
    "    inference_network=flow,\n",
    "    inference_variables=[\"inference_variables\"],\n",
    "    summary_variables=[\"summary_variables\"]\n",
    ")\n",
    "\n",
    "wf.approximator = loaded_approx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e92cc0",
   "metadata": {},
   "source": [
    "Making parameter estimates for the test datasets. The test datasets and true parameter values are generated in the notebook ezbf_results.ipynb, to ensure that the same thetas and datasets are used in both notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e4adde",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100        \n",
    "n_trials = 500  \n",
    "n_samples = 500 \n",
    "param_names = ['vL', 'vR', 'a', 'z', 'terL', 'terR']\n",
    "\n",
    "# Load stored sets from ez_results.ipynb\n",
    "# These sets were generated to ensure the same thetas are used for both models to obtain comparable results\n",
    "with open(\"set_thetas.pkl\", \"rb\") as f:\n",
    "    set_thetas = pickle.load(f)\n",
    "\n",
    "obs_data = np.load(\"set_test_data.npy\")\n",
    "\n",
    "true_params = []\n",
    "post_samples = []\n",
    "\n",
    "for i in range(N):\n",
    "    theta = set_thetas[i]\n",
    "    true_params.append([theta[k] for k in param_names])\n",
    "\n",
    "    # Extract components from raw data\n",
    "    rt = obs_data[i, :, 0]\n",
    "    stimulus = obs_data[i, :, 1]\n",
    "    choice = obs_data[i, :, 2]\n",
    "\n",
    "    # Prepare input for BayesFlow, where keys must match adapter expectations\n",
    "    input_data = {\n",
    "        \"rts\": rt[np.newaxis, :],\n",
    "        \"stimulus\": stimulus[np.newaxis, :],\n",
    "        \"choices\": choice[np.newaxis, :]\n",
    "    }\n",
    "\n",
    "    samples = wf.sample(conditions=input_data, num_samples=n_samples, to_numpy=True)\n",
    "\n",
    "    param_array = np.stack([\n",
    "        samples[\"vL\"], samples[\"vR\"],\n",
    "        samples[\"a\"], samples[\"z\"],\n",
    "        samples[\"terL\"], samples[\"terR\"]\n",
    "    ], axis=-1)\n",
    "\n",
    "    param_array = np.squeeze(param_array, axis=(0, 2))\n",
    "\n",
    "    post_samples.append(param_array)\n",
    "\n",
    "# Convert to arrays\n",
    "true_params = np.array(true_params)   # shape: (N, 6)\n",
    "post_samples = np.array(post_samples) # shape: (N, n_samples, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4949ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definining a new prior to be used to generate the simulated test data\n",
    "# This prior is based on realistic, empirical parameter distributions described\n",
    "# in Tran et al. (2021)\n",
    "\n",
    "def sample_trunc_t(df, loc, scale, lower, upper):\n",
    "    while True:\n",
    "        x = np.random.standard_t(df=df)\n",
    "        y = loc + scale * x\n",
    "        if lower <= y <= upper:\n",
    "            return y\n",
    "\n",
    "def plausible_prior():\n",
    "    # Drift rates: Normal(1.76, 1.51) truncated to ≥ 0.2\n",
    "    while True:\n",
    "        vL = np.random.normal(loc=1.76, scale=1.51)\n",
    "        vR = np.random.normal(loc=1.76, scale=1.51)\n",
    "        if vL >= 0.2 and vR >= 0.2:\n",
    "            break\n",
    "\n",
    "    # Boundary separation: Gamma(11.69, 0.12) capped at 4\n",
    "    a = np.random.gamma(shape=11.69, scale=0.12)\n",
    "    a = min(a, 4)\n",
    "\n",
    "    # Starting point: Truncated Student-T in [0, 1]\n",
    "    z = sample_trunc_t(df=1.85, loc=0.5, scale=0.1, lower=0.0, upper=1.0)\n",
    "\n",
    "    # Non-decision times: Truncated Student-T ≥ 0\n",
    "    while True:\n",
    "        terL = sample_trunc_t(df=1.32, loc=0.44, scale=0.08, lower=0.0, upper=np.inf)\n",
    "        terR = sample_trunc_t(df=1.32, loc=0.44, scale=0.08, lower=0.0, upper=np.inf)\n",
    "        if terL >= 0 and terR >= 0:\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        \"vL\": vL,\n",
    "        \"vR\": vR,\n",
    "        \"a\": a,\n",
    "        \"z\": z,\n",
    "        \"terL\": terL,\n",
    "        \"terR\": terR\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d131b3d7",
   "metadata": {},
   "source": [
    "Computing the posterior calibration for the boundary separation (a) parameter. The calibration is computed on an independent set of simulated data, which uses the plausible prior truncated to the ranges of the uniform prior used in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52d1200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating data for the calibration\n",
    "\n",
    "# Defining the parameter ranges used in model training\n",
    "TRAIN_SUPPORT = {\n",
    "    \"vL\":   (0.1, 6.0),\n",
    "    \"vR\":   (0.1, 6.0),\n",
    "    \"a\":    (0.3, 4.0),\n",
    "    \"z\":    (0.1, 0.9),\n",
    "    \"terL\": (0.1, 1.0),\n",
    "    \"terR\": (0.1, 1.0),\n",
    "}\n",
    "\n",
    "N_CAL         = 200\n",
    "N_TRIALS_CAL  = 400\n",
    "SAMPLES_CAL   = 500\n",
    "SEED_CAL      = 123\n",
    "\n",
    "rng = np.random.default_rng(SEED_CAL)\n",
    "\n",
    "cal_true_params   = []\n",
    "cal_post_samples  = []\n",
    "cal_skip_log      = []\n",
    "accepted = 0\n",
    "candidates = 0\n",
    "MAX_CAND = 2000\n",
    "\n",
    "while accepted < N_CAL and candidates < MAX_CAND:\n",
    "    candidates += 1\n",
    "    theta = plausible_prior()\n",
    "\n",
    "    # Keep only thetas inside the training support\n",
    "    if any(not (lo <= theta[k] <= hi) for k,(lo,hi) in TRAIN_SUPPORT.items()):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        sim = forward_model_ez2(**theta, n_trials=N_TRIALS_CAL, rng=rng)\n",
    "        rts_log = np.asarray(sim[\"rts\"], float)\n",
    "        stim    = np.asarray(sim[\"stimulus\"], int)\n",
    "        choice  = np.asarray(sim[\"choices\"], int)\n",
    "\n",
    "        # Quick safety checks\n",
    "        if not (np.isfinite(rts_log).all() and rts_log.size == stim.size == choice.size):\n",
    "            cal_skip_log.append({\"reason\": \"non-finite or size mismatch\", \"theta\": theta})\n",
    "            continue\n",
    "        if not (np.any(stim == 0) and np.any(stim == 1)):\n",
    "            cal_skip_log.append({\"reason\": \"missing one stimulus\", \"theta\": theta})\n",
    "            continue\n",
    "\n",
    "        input_data = {\n",
    "            \"rts\":      rts_log[np.newaxis, :],\n",
    "            \"stimulus\": stim[np.newaxis, :],\n",
    "            \"choices\":  choice[np.newaxis, :],\n",
    "        }\n",
    "\n",
    "        samples = wf.sample(conditions=input_data, num_samples=SAMPLES_CAL, to_numpy=True)\n",
    "\n",
    "        post_array = np.column_stack([\n",
    "            samples[\"vL\"][0].squeeze(),\n",
    "            samples[\"vR\"][0].squeeze(),\n",
    "            samples[\"a\"][0].squeeze(),\n",
    "            samples[\"z\"][0].squeeze(),\n",
    "            samples[\"terL\"][0].squeeze(),\n",
    "            samples[\"terR\"][0].squeeze()\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "        # Safety check\n",
    "        if not np.isfinite(post_array).all():\n",
    "            cal_skip_log.append({\"reason\": \"NaNs in posterior\", \"theta\": theta})\n",
    "            continue\n",
    "\n",
    "    # Log thetas that had to be skipped (if any)\n",
    "    except Exception as e:\n",
    "        cal_skip_log.append({\"reason\": f\"Exception: {e}\", \"theta\": theta})\n",
    "        continue\n",
    "\n",
    "    cal_true_params.append([theta[k] for k in [\"vL\",\"vR\",\"a\",\"z\",\"terL\",\"terR\"]])\n",
    "    cal_post_samples.append(post_array)\n",
    "    accepted += 1\n",
    "\n",
    "print(f\"Calibration split: accepted {accepted} from {candidates} candidates.\")\n",
    "cal_true_params  = np.asarray(cal_true_params, dtype=float)       # (N_CAL, 6)\n",
    "cal_post_samples = np.asarray(cal_post_samples, dtype=np.float32) # (N_CAL, SAMPLES_CAL, 6)\n",
    "\n",
    "\n",
    "# Computing the calibration\n",
    "\n",
    "PARAM_ORDER = ['vL','vR','a','z','terL','terR']\n",
    "\n",
    "# fitting: piecewise inverse for 'a' on the independent calibration split\n",
    "def fit_piecewise_a(cal_true_params, cal_post_samples, param_order,\n",
    "                    nbins=10, min_bin_n=8, point_est=\"mean\"):\n",
    "    j = param_order.index(\"a\")\n",
    "    est_mat = np.median(cal_post_samples, 1) if point_est==\"median\" else np.mean(cal_post_samples, 1)\n",
    "    x = cal_true_params[:, j].astype(float)   # truth\n",
    "    y = est_mat[:, j].astype(float)           # posterior mean (raw)\n",
    "\n",
    "    # quantile bins on truth\n",
    "    bins = np.quantile(x, np.linspace(0, 1, nbins + 1))\n",
    "    idx  = np.digitize(x, bins[1:-1], right=True)\n",
    "\n",
    "    xs, ys = [], []\n",
    "    for b in range(nbins):\n",
    "        m = (idx == b)\n",
    "        if m.sum() >= min_bin_n:\n",
    "            xs.append(float(x[m].mean()))\n",
    "            ys.append(float(y[m].mean()))\n",
    "\n",
    "    if len(xs) < 3:\n",
    "        return {\"mode\": \"identity\"}\n",
    "\n",
    "    xs = np.asarray(xs, float)\n",
    "    ys = np.asarray(ys, float)\n",
    "\n",
    "    # ensure monotone in y for safe interpolation; collapse near-duplicates\n",
    "    order = np.argsort(ys)\n",
    "    ys = ys[order]; xs = xs[order]\n",
    "    uniq = np.concatenate(([True], np.diff(ys) > 1e-12))\n",
    "    ys = ys[uniq]; xs = xs[uniq]\n",
    "    if ys.size < 3:\n",
    "        return {\"mode\": \"identity\"}\n",
    "\n",
    "    return {\"mode\": \"piecewise\", \"ys\": ys, \"xs\": xs}\n",
    "\n",
    "# apply to calibrate only 'a', since prior investigation found the calibration to not improve the MSE of vL and vR\n",
    "def apply_piecewise_a_only(post_samples, model, param_order):\n",
    "    if model.get(\"mode\") != \"piecewise\":\n",
    "        return post_samples  # identity\n",
    "\n",
    "    j = param_order.index(\"a\")\n",
    "    ys, xs = model[\"ys\"], model[\"xs\"]\n",
    "\n",
    "    out = post_samples.copy()\n",
    "    y = out[..., j].astype(np.float64)                        # raw draws of 'a'\n",
    "    x_hat = np.interp(y, ys, xs, left=xs[0], right=xs[-1])    # inverse map g(y)\n",
    "    x_hat = np.clip(x_hat, 1e-4, None)                        # 'a' must be > 0\n",
    "    out[..., j] = x_hat.astype(out.dtype, copy=False)\n",
    "    return out\n",
    "\n",
    "# fit on the calibration data and apply to the test data\n",
    "a_model = fit_piecewise_a(cal_true_params, cal_post_samples, PARAM_ORDER,\n",
    "                          nbins=10, min_bin_n=8, point_est=\"mean\")\n",
    "\n",
    "post_samples_cal = apply_piecewise_a_only(post_samples, a_model, PARAM_ORDER)\n",
    "\n",
    "# quick sanity on TEST (posterior mean as estimator)\n",
    "def _est(arr, how=\"mean\"): return np.median(arr,1) if how==\"median\" else np.mean(arr,1)\n",
    "j_a = PARAM_ORDER.index(\"a\")\n",
    "b0 = float(np.mean(_est(post_samples,       \"mean\")[:, j_a] - true_params[:, j_a]))\n",
    "b1 = float(np.mean(_est(post_samples_cal,  \"mean\")[:, j_a] - true_params[:, j_a]))\n",
    "print(f\"[TEST] a: mean bias {b0:.3f} -> {b1:.3f}\")\n",
    "\n",
    "# quick safety tests\n",
    "# after creating post_samples_cal\n",
    "assert post_samples.shape == post_samples_cal.shape\n",
    "j = PARAM_ORDER.index(\"a\")\n",
    "# calibration changes only 'a'\n",
    "unchanged = np.allclose(post_samples[..., np.r_[0:j, j+1:6]],\n",
    "                        post_samples_cal[..., np.r_[0:j, j+1:6]])\n",
    "assert unchanged, \"Only 'a' should change under calibration.\"\n",
    "\n",
    "# function for calibrating a dict of posterior samples (used in the validation study)\n",
    "def calibrate_bf_sample_dict_a_only(samples, a_model, param_order):\n",
    "    if a_model.get(\"mode\") != \"piecewise\":\n",
    "        return samples\n",
    "    arr = np.column_stack([\n",
    "        samples['vL'][0].squeeze(),\n",
    "        samples['vR'][0].squeeze(),\n",
    "        samples['a'][0].squeeze(),\n",
    "        samples['z'][0].squeeze(),\n",
    "        samples['terL'][0].squeeze(),\n",
    "        samples['terR'][0].squeeze()\n",
    "    ]).astype(np.float32)[np.newaxis, ...]\n",
    "    \n",
    "    arr_cal = apply_piecewise_a_only(arr, a_model, param_order)[0]\n",
    "\n",
    "    out = {k: v.copy() for k,v in samples.items()}\n",
    "    out['a'][0] = arr_cal[:, 2][:, None]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c7797e",
   "metadata": {},
   "source": [
    "The whether the posterior samples including the calibrated 'a' are used can be toggled using the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fa640a",
   "metadata": {},
   "outputs": [],
   "source": [
    "APPLY_CALIBRATION = False # toggle for using the calibration (True) or not using it (False)\n",
    "post_samples_used = (post_samples_cal if APPLY_CALIBRATION else post_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d36631c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy metrics\n",
    "\n",
    "# Optional: toggle which point estimate is used for calculating errors\n",
    "POINT_EST = \"mean\" # \"mean\" or \"median\"\n",
    "\n",
    "param_names = ['vL', 'vR', 'a', 'z', 'terL', 'terR']\n",
    "\n",
    "records = []\n",
    "for i in range(true_params.shape[0]):\n",
    "    for j, name in enumerate(param_names):\n",
    "        true_val = float(true_params[i, j])\n",
    "        samples  = post_samples_used[i, :, j]\n",
    "\n",
    "        # Posterior summaries\n",
    "        post_mean = float(np.mean(samples))\n",
    "        post_median = float(np.median(samples))\n",
    "        post_var = float(np.var(samples, ddof=1))\n",
    "        post_sd  = float(np.sqrt(post_var))\n",
    "\n",
    "        # Point estimate used for accuracy metrics\n",
    "        point_est = post_median if POINT_EST == \"median\" else post_mean\n",
    "\n",
    "        # Per-dataset estimation error (bias at the single-dataset level)\n",
    "        error = float(point_est - true_val)\n",
    "        se    = float(error**2)                      # squared error (per dataset)\n",
    "\n",
    "        records.append({\n",
    "            \"dataset\": i,\n",
    "            \"parameter\": name,\n",
    "            \"true_value\": true_val,\n",
    "            \"posterior_mean\": post_mean,\n",
    "            \"posterior_median\": post_median,\n",
    "            \"posterior_variance\": post_var,\n",
    "            \"posterior_sd\": post_sd,\n",
    "            \"point_estimate\": point_est,            # the estimator used for triad\n",
    "            \"error\": error,                         # will aggregate to Bias = mean(error)\n",
    "            \"mse\": se                               # per-dataset squared error; MSE = mean(mse)\n",
    "        })\n",
    "\n",
    "performance_stats_df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab26b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "triad = (\n",
    "    performance_stats_df\n",
    "    .groupby('parameter')\n",
    "    .agg(\n",
    "        n=('error','size'),\n",
    "        Bias=('error','mean'),\n",
    "        Variance_of_error=('error', lambda s: s.var(ddof=1)),\n",
    "        MSE=('mse','mean')\n",
    "    ).reset_index()\n",
    ")\n",
    "\n",
    "triad.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8635276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying and cleaning outliers\n",
    "thr = performance_stats_df.groupby('parameter')['error'].transform(lambda s: s.abs().quantile(0.75) + 3.0*(s.abs().quantile(0.75)-s.abs().quantile(0.25)))\n",
    "df_clean = performance_stats_df[performance_stats_df['error'].abs() <= thr].copy()\n",
    "\n",
    "is_out = ~performance_stats_df.index.isin(df_clean.index)\n",
    "outlier_counts = performance_stats_df.assign(outlier=is_out).groupby('parameter')['outlier'].agg(n_total='size', n_dropped='sum').assign(pct_dropped=lambda x: 100*x['n_dropped']/x['n_total'])\n",
    "outlier_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2377630e",
   "metadata": {},
   "outputs": [],
   "source": [
    "triad_clean = (\n",
    "    df_clean\n",
    "    .groupby('parameter')\n",
    "    .agg(\n",
    "        n=('error','size'),\n",
    "        Bias=('error','mean'),\n",
    "        Variance=('error', lambda s: s.var(ddof=1)),   # variance of error across datasets\n",
    "        MSE=('error', lambda s: float(np.mean(s**2)))  # mean squared error across datasets\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "triad_clean.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fa15f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing output for later model comparison\n",
    "\n",
    "MODEL_LABEL = \"standard\"\n",
    "\n",
    "performance_stats_df = performance_stats_df.copy()\n",
    "performance_stats_df[\"model\"] = MODEL_LABEL\n",
    "df_clean = df_clean.copy()\n",
    "df_clean[\"model\"] = MODEL_LABEL\n",
    "\n",
    "performance_stats_df.to_csv(f\"{MODEL_LABEL}_perf_full.csv\", index=False)\n",
    "df_clean.to_csv(f\"{MODEL_LABEL}_perf_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1225bf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the bias and posterior variance\n",
    "\n",
    "import seaborn as sns, matplotlib.pyplot as plt, numpy as np, pandas as pd\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "DFU = performance_stats_df.copy()\n",
    "base = performance_stats_df\n",
    "\n",
    "# counts dropped per parameter\n",
    "n_total = base.groupby(\"parameter\").size()\n",
    "n_kept  = DFU.groupby(\"parameter\").size()\n",
    "n_drop  = (n_total - n_kept).reindex(n_total.index).fillna(0).astype(int)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Panel A: estimation error\n",
    "sns.boxplot(data=DFU, x=\"parameter\", y=\"error\", ax=axes[0], showfliers=False)\n",
    "axes[0].axhline(0, ls=\"--\", c=\"gray\", lw=1)\n",
    "axes[0].set_ylim(-1.6, 1.9)\n",
    "axes[0].set_title(\"Bias by Parameter\")\n",
    "axes[0].set_xlabel(\"\"); axes[0].set_ylabel(\"Bias\")\n",
    "\n",
    "# Panel B: posterior variance\n",
    "sns.boxplot(data=DFU, x=\"parameter\", y=\"posterior_sd\", ax=axes[1], showfliers=False)\n",
    "axes[1].set_ylim(0, 0.6)\n",
    "axes[1].set_title(\"Posterior SD by Parameter\")\n",
    "axes[1].set_xlabel(\"\"); axes[1].set_ylabel(\"Posterior SD\")\n",
    "\n",
    "plt.tight_layout(rect=[0,0.05,1,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac10d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credible intervals and coverage\n",
    "\n",
    "N, S, P = post_samples_used.shape  # N = datasets, S = samples, P = parameters\n",
    "\n",
    "ci_records = []\n",
    "\n",
    "for i in range(N):\n",
    "    for j, name in enumerate(param_names):\n",
    "        samples_ij = post_samples_used[i, :, j]\n",
    "        true_val = true_params[i, j]\n",
    "\n",
    "        # Compute 95% credible interval\n",
    "        lower = np.percentile(samples_ij, 2.5)\n",
    "        upper = np.percentile(samples_ij, 97.5)\n",
    "        width = upper - lower\n",
    "\n",
    "        # Coverage\n",
    "        covered = int(lower <= true_val <= upper)\n",
    "\n",
    "        ci_records.append({\n",
    "            \"dataset\": i,\n",
    "            \"parameter\": name,\n",
    "            \"true_value\": true_val,\n",
    "            \"lower_95\": lower,\n",
    "            \"upper_95\": upper,\n",
    "            \"width_95\": width,\n",
    "            \"covered\": covered\n",
    "        })\n",
    "\n",
    "ci_df = pd.DataFrame(ci_records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79614667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall coverage per parameter\n",
    "coverage_summary = ci_df.groupby(\"parameter\")[\"covered\"].mean().reset_index()\n",
    "coverage_summary.rename(columns={\"covered\": \"coverage_rate\"}, inplace=True)\n",
    "\n",
    "# Plot distribution of credible interval widths\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=ci_df, x=\"parameter\", y=\"width_95\")\n",
    "plt.title(\"Distribution of 95% CrI Widths by Parameter\")\n",
    "plt.ylabel(\"Width of 95% CrI\")\n",
    "plt.xlabel(\"Parameter\")\n",
    "plt.ylim(0, 2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Return the summary coverage table\n",
    "coverage_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76513200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing output for later model comparison\n",
    "\n",
    "MODEL_LABEL = \"standard\"\n",
    "ci_df_model = ci_df.copy()\n",
    "ci_df_model[\"model\"] = MODEL_LABEL\n",
    "ci_df_model.to_csv(f\"{MODEL_LABEL}_ci_full.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c32773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation-based calibration (SBC)\n",
    "\n",
    "from scipy.stats import chi2, kstest, binom\n",
    "\n",
    "N, S, P = post_samples_used.shape\n",
    "rng = np.random.default_rng(1312)\n",
    "\n",
    "# Randomized ranks and normalized ranks z in (0,1)\n",
    "sbc_records = []\n",
    "for j, name in enumerate(param_names):\n",
    "    for i in range(N):\n",
    "        true_val = float(true_params[i, j])\n",
    "        samples  = post_samples_used[i, :, j]\n",
    "\n",
    "        n_less  = np.sum(samples < true_val)\n",
    "        n_equal = np.sum(samples == true_val)   # robust to rare ties\n",
    "        rank    = n_less + rng.uniform(0, 1) * n_equal     # fractional rank\n",
    "        z       = (rank + 1.0) / (S + 1.0)                 # normalized (Uniform(0,1) target)\n",
    "\n",
    "        sbc_records.append({\"dataset\": i, \"parameter\": name, \"rank\": float(rank), \"z\": float(z)})\n",
    "\n",
    "sbc_df = pd.DataFrame(sbc_records)\n",
    "\n",
    "# KL divergence helper (observed || uniform), with Jeffreys smoothing\n",
    "def kl_to_uniform(counts, alpha=0.5, base='e'):\n",
    "    counts = np.asarray(counts, float)\n",
    "    B = len(counts)\n",
    "    p = (counts + alpha) / (counts.sum() + alpha * B)  # smoothed observed probs\n",
    "    q = np.full(B, 1.0 / B)                            # exact uniform probs\n",
    "    kl = np.sum(p * np.log(p / q))\n",
    "    if base == '2':\n",
    "        kl /= np.log(2.0)\n",
    "    return float(max(kl, 0.0))\n",
    "\n",
    "# Histograms with 95% expected bands + diagnostics\n",
    "num_bins = 20\n",
    "bins = np.linspace(0.0, 1.0, num_bins + 1)\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(10, 12), sharey=False)\n",
    "axes = axes.flatten()\n",
    "\n",
    "sbc_summ_rows = []\n",
    "\n",
    "for ax, name in zip(axes, param_names):\n",
    "    sub = sbc_df.loc[sbc_df[\"parameter\"] == name, \"z\"].to_numpy()\n",
    "    counts, _ = np.histogram(sub, bins=bins)\n",
    "    n_sub = len(sub)\n",
    "    exp = n_sub / num_bins\n",
    "\n",
    "    # 95% band for each bin's count under Binomial(n_sub, 1/B)\n",
    "    low, high = binom.interval(0.95, n_sub, 1/num_bins)\n",
    "    low, high = float(low), float(high)\n",
    "\n",
    "    # diagnostics\n",
    "    kl = kl_to_uniform(counts, alpha=0.5, base='e')\n",
    "    chi_stat = float(((counts - exp)**2 / exp).sum())\n",
    "    chi_p = float(chi2.sf(chi_stat, df=num_bins - 1))\n",
    "\n",
    "    sbc_summ_rows.append({\n",
    "        \"parameter\": name, \"n\": n_sub,\n",
    "        \"kl_div_to_uniform\": kl,\n",
    "        \"chi2_stat\": chi_stat, \"chi2_p\": chi_p\n",
    "    })\n",
    "\n",
    "    ax.bar(np.arange(num_bins), counts, width=1, edgecolor='k')\n",
    "    ax.axhline(exp, color='red', linestyle='--', lw=1, label='expected')\n",
    "    ax.axhspan(low, high, color='lightgray', alpha=0.4, zorder=0, label='95% band')\n",
    "    ax.set_title(f\"{name}\\nKL={kl:.3f}  χ²p={chi_p:.3f}\")\n",
    "    ax.set_xlabel(\"rank bin\"); ax.set_ylabel(\"count\")\n",
    "    ax.set_xlim(-0.5, num_bins - 0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "sbc_summary = pd.DataFrame(sbc_summ_rows).sort_values(\"parameter\")\n",
    "display(sbc_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579e79fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPC with symmetric KL on counts + Jeffreys smoothing\n",
    "\n",
    "n_sim_samples = 50\n",
    "n_trials_per_sim = 100\n",
    "\n",
    "kl_records = []\n",
    "skipped_logs = []\n",
    "\n",
    "def sym_kl_counts(obs_counts, sim_counts, alpha=0.5):\n",
    "    obs_counts = np.asarray(obs_counts, float)\n",
    "    sim_counts = np.asarray(sim_counts, float)\n",
    "    B = len(obs_counts)\n",
    "    p = (obs_counts + alpha) / (obs_counts.sum() + alpha * B)\n",
    "    q = (sim_counts + alpha) / (sim_counts.sum() + alpha * B)\n",
    "    kl_pq = np.sum(p * np.log(p / q))\n",
    "    kl_qp = np.sum(q * np.log(q / p))\n",
    "    return 0.5 * (kl_pq + kl_qp)\n",
    "\n",
    "for i in range(post_samples_used.shape[0]):\n",
    "    obs_rts = obs_data[i, :, 0]\n",
    "    sim_rts = []\n",
    "    skipped = 0\n",
    "\n",
    "    # choose posterior indices\n",
    "    idxs = np.random.choice(post_samples_used.shape[1], size=n_sim_samples, replace=False)\n",
    "\n",
    "    for idx in idxs:\n",
    "        vL, vR, a, z, terL, terR = post_samples_used[i, idx]\n",
    "\n",
    "        # parameter sanity checks and logging of errors\n",
    "        if any([vL <= 0, vR <= 0, a <= 0, not (0 < z < 1), terL < 0, terR < 0]):\n",
    "            skipped_logs.append({\n",
    "                \"dataset\": i, \"sample_index\": idx,\n",
    "                \"vL\": vL, \"vR\": vR, \"a\": a, \"z\": z, \"terL\": terL, \"terR\": terR,\n",
    "                \"reason\": \"invalid parameter values\"\n",
    "            })\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            sim = forward_model_ez2(vL=vL, vR=vR, a=a, z=z, terL=terL, terR=terR,\n",
    "                                    n_trials=n_trials_per_sim)\n",
    "            rts = np.asarray(sim[\"rts\"], float)\n",
    "            # guard against nan/inf\n",
    "            rts = rts[np.isfinite(rts)]\n",
    "            if rts.size:\n",
    "                sim_rts.extend(rts)\n",
    "        except Exception as e:\n",
    "            skipped_logs.append({\n",
    "                \"dataset\": i, \"sample_index\": idx,\n",
    "                \"vL\": vL, \"vR\": vR, \"a\": a, \"z\": z, \"terL\": terL, \"terR\": terR,\n",
    "                \"reason\": str(e)\n",
    "            })\n",
    "            skipped += 1\n",
    "\n",
    "    sim_rts = np.asarray(sim_rts, float)\n",
    "\n",
    "    if sim_rts.size == 0:\n",
    "        kl_div = np.nan\n",
    "    else:\n",
    "        # shared bins from pooled data (Freedman–Diaconis; FD)\n",
    "        pooled = np.concatenate([obs_rts, sim_rts])\n",
    "        pooled = pooled[np.isfinite(pooled)]\n",
    "        bin_edges = np.histogram_bin_edges(pooled, bins='fd')\n",
    "        # fallback if FD returns too few bins:\n",
    "        if len(bin_edges) < 5:\n",
    "            lo, hi = np.nanmin(pooled), np.nanmax(pooled)\n",
    "            bin_edges = np.linspace(lo, hi, 21)\n",
    "\n",
    "        obs_counts, _ = np.histogram(obs_rts, bins=bin_edges, density=False)\n",
    "        sim_counts, _ = np.histogram(sim_rts, bins=bin_edges, density=False)\n",
    "\n",
    "        kl_div = sym_kl_counts(obs_counts, sim_counts, alpha=0.5)\n",
    "\n",
    "    kl_records.append({\n",
    "        \"dataset\": i,\n",
    "        \"kl_divergence\": float(kl_div),\n",
    "        \"skipped_samples\": int(skipped),\n",
    "        \"n_sim_rts\": int(sim_rts.size)\n",
    "    })\n",
    "\n",
    "kl_ppc_df = pd.DataFrame(kl_records)\n",
    "skipped_df = pd.DataFrame(skipped_logs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebe3651",
   "metadata": {},
   "source": [
    "Not all samples of a parameter's posterior are within valid parameter bounds, such as negative values. If the PPC code above selects negative samples, the forward model breaks, since it expects positive inputs. The dataframe below shows that these parameter sets were skipped due to the presence of invalid (negative) parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e1c82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "skipped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7969547c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_ppc_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0695f30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "sns.boxplot(y=kl_ppc_df[\"kl_divergence\"], width=0.35, showfliers=False)\n",
    "plt.ylabel(\"Symmetric KL\")\n",
    "plt.grid(True, axis=\"y\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce735f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function and toggle for applying a log-transform on RTs\n",
    "# this is required for the standard model, since the real experimental data\n",
    "# is on a regular scale, while the model was trained on log-transformed data\n",
    "\n",
    "def apply_rt_transform(x, how):\n",
    "    if how in (None, \"none\"): return x.astype(np.float32)\n",
    "    if how == \"log1p\":\n",
    "        if np.any(x < 0): raise ValueError(\"Negative RT encountered with log1p.\")\n",
    "        return np.log1p(x.astype(np.float32))\n",
    "    raise ValueError(f\"Unknown RT transform: {how}\")\n",
    "\n",
    "RT_TRANSFORM = \"log1p\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28386eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the validation data\n",
    "\n",
    "def load_validation_data_by_subject(base_path):\n",
    "    \"\"\"\n",
    "    Reads\n",
    "      base_path/exp_1/pp1.txt ... pp20.txt,\n",
    "      ...\n",
    "      base_path/exp_14/pp1.txt ... pp20.txt\n",
    "\n",
    "    Returns: list of dicts:\n",
    "      {\"dataset_id\": i, \"participants\": [\n",
    "          {\"subject_id\": \"pp1\",\n",
    "           \"A\": {\"rts\":..., \"stimulus\":..., \"response\":..., \"correct\":...},\n",
    "           \"B\": {...}},\n",
    "          ...\n",
    "      ]}\n",
    "    \"\"\"\n",
    "\n",
    "    datasets = []\n",
    "\n",
    "    for i in range(1, 15):  # exp_1 ... exp_14\n",
    "        exp_path = os.path.join(base_path, f\"exp_{i}\")\n",
    "        participants = []\n",
    "\n",
    "        for j in range(1, 21):  # pp1 ... pp20\n",
    "            df = pd.read_csv(os.path.join(exp_path, f\"pp{j}.txt\"), sep=r\"\\s+\", engine=\"python\")\n",
    "            df.columns = [c.lower() for c in df.columns]\n",
    "\n",
    "            df[\"stimulus\"] = df[\"stim\"].map({\"L\":0,\"R\":1}).astype(\"int64\")\n",
    "            df[\"response\"] = df[\"resp\"].map({\"L\":0,\"R\":1}).astype(\"int64\")\n",
    "            df[\"correct\"]  = df[\"correct\"].astype(\"int64\")\n",
    "            rt_raw = df[\"rt\"].astype(\"float32\").to_numpy()\n",
    "            rt_for_model = apply_rt_transform(rt_raw, RT_TRANSFORM)\n",
    "\n",
    "            subj = {\"subject_id\": f\"pp{j}\"}\n",
    "            for cond in [\"A\",\"B\"]:\n",
    "                idx = df.index[df[\"cond\"]==cond].to_numpy()\n",
    "                subj[cond] = {\n",
    "                    \"rts\":      rt_for_model[idx],\n",
    "                    \"stimulus\": df.loc[idx,\"stimulus\"].to_numpy(np.int64),\n",
    "                    \"response\": df.loc[idx,\"response\"].to_numpy(np.int64),\n",
    "                    \"correct\":  df.loc[idx,\"correct\"].to_numpy(np.int64),\n",
    "                }\n",
    "\n",
    "            participants.append(subj)\n",
    "        datasets.append({\"dataset_id\": i, \"participants\": participants})\n",
    "    return datasets\n",
    "\n",
    "base = r\"C:/Users/emils/Documents/uni/M_Thesis/diffusion-bayesflow/data/real/validation_text_data/validation_text_data\"\n",
    "val_data_by_subj = load_validation_data_by_subject(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d93b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining posteriors for each subject per condition\n",
    "\n",
    "def run_inference_per_subject(wf, datasets, num_samples=500):\n",
    "    \"\"\"\n",
    "    Returns: [ {\"dataset_id\": i,\n",
    "                \"subjects\": [{\"subject_id\": \"...\",\n",
    "                              \"A_samples\": (num_samples, 6) or None,\n",
    "                              \"B_samples\": (num_samples, 6) or None, ...}, ...]} ]\n",
    "    \"\"\"\n",
    "    out = []\n",
    "\n",
    "    for ds in datasets:\n",
    "        dsid = ds[\"dataset_id\"]\n",
    "        subj_results = []\n",
    "\n",
    "        for subj in ds[\"participants\"]:\n",
    "            sid = subj[\"subject_id\"]\n",
    "            rec = {\"subject_id\": sid}\n",
    "\n",
    "            for cond in [\"A\", \"B\"]:\n",
    "                cd = subj[cond]\n",
    "                if cd[\"rts\"].size == 0:\n",
    "                    rec[f\"{cond}_samples\"] = None\n",
    "                    rec[f\"{cond}_error\"] = \"no trials\"\n",
    "                    continue\n",
    "\n",
    "                input_data = {\n",
    "                    \"rts\":      cd[\"rts\"][np.newaxis, :],\n",
    "                    \"stimulus\": cd[\"stimulus\"][np.newaxis, :],\n",
    "                    \"choices\":  cd[\"response\"][np.newaxis, :],\n",
    "                }\n",
    "\n",
    "                try:\n",
    "                    samples = wf.sample(conditions=input_data, num_samples=num_samples, to_numpy=True)\n",
    "                    # Optional: apply calibration if toggled\n",
    "                    if APPLY_CALIBRATION:\n",
    "                        samples = calibrate_bf_sample_dict_a_only(samples, a_model, PARAM_ORDER)\n",
    "                    param_array = np.column_stack([\n",
    "                        samples['vL'][0].squeeze(),\n",
    "                        samples['vR'][0].squeeze(),\n",
    "                        samples['a'][0].squeeze(),\n",
    "                        samples['z'][0].squeeze(),\n",
    "                        samples['terL'][0].squeeze(),\n",
    "                        samples['terR'][0].squeeze()\n",
    "                    ]).astype(np.float32)\n",
    "                    rec[f\"{cond}_samples\"] = param_array\n",
    "                except Exception as e:\n",
    "                    rec[f\"{cond}_samples\"] = None\n",
    "                    rec[f\"{cond}_error\"] = str(e)\n",
    "\n",
    "            subj_results.append(rec)\n",
    "\n",
    "        out.append({\"dataset_id\": dsid, \"subjects\": subj_results})\n",
    "    return out\n",
    "\n",
    "post_by_subj = run_inference_per_subject(wf, val_data_by_subj, num_samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cf7c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing condition differences by parameter, on subject-level\n",
    "\n",
    "def make_subject_deltas(post_by_subj):\n",
    "    \"\"\"\n",
    "    Returns a flat list of rows:\n",
    "      {\"dataset_id\": i, \"subject_id\": sid,\n",
    "       \"delta\": {\"v\": (K,), \"a\": (K,), \"z\": (K,), \"ter\": (K,)} }\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for ds in post_by_subj:\n",
    "        dsid = ds[\"dataset_id\"]\n",
    "        for s in ds[\"subjects\"]:\n",
    "            A, B = s.get(\"A_samples\"), s.get(\"B_samples\")\n",
    "            if A is None or B is None:\n",
    "                continue\n",
    "\n",
    "            vA, vB   = (A[:,0] + A[:,1]), (B[:,0] + B[:,1])\n",
    "            aA, aB   = A[:,2], B[:,2]\n",
    "            zA, zB   = np.log(1 - A[:,3]), np.log(1 - B[:,3])\n",
    "            terA, terB = (A[:,4] + A[:,5]), (B[:,4] + B[:,5])\n",
    "\n",
    "            rows.append({\n",
    "                \"dataset_id\": dsid,\n",
    "                \"subject_id\": s[\"subject_id\"],\n",
    "                \"delta\": {\"v\": vB - vA, \"a\": aB - aA, \"z\": zB - zA, \"ter\": terB - terA},\n",
    "            })\n",
    "    return rows\n",
    "\n",
    "subj_deltas = make_subject_deltas(post_by_subj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf98bae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining group-level condition differences\n",
    "\n",
    "def aggregate_group_mean(subj_deltas, num_draws=4000, seed=123):\n",
    "    \"\"\"\n",
    "    Composition sampling:\n",
    "      For each dataset & parameter:\n",
    "        draw one index from each subject's Δ-samples, average -> one μ draw.\n",
    "    Returns: [{\"dataset_id\": i, \"v_samples_group\": (M,), \"a_samples_group\":..., ...}, ...]\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # collect per dataset/param\n",
    "    grouped = {}\n",
    "    for row in subj_deltas:\n",
    "        dsid = row[\"dataset_id\"]\n",
    "        grouped.setdefault(dsid, {p: [] for p in [\"v\",\"a\",\"z\",\"ter\"]})\n",
    "        for p in [\"v\",\"a\",\"z\",\"ter\"]:\n",
    "            grouped[dsid][p].append(row[\"delta\"][p])\n",
    "\n",
    "    group_samples = []\n",
    "    for dsid, by_param in grouped.items():\n",
    "        rec = {\"dataset_id\": dsid}\n",
    "        for p, arrs in by_param.items():\n",
    "            if len(arrs) == 0:\n",
    "                rec[f\"{p}_samples_group\"] = None\n",
    "                continue\n",
    "            K = arrs[0].shape[0]\n",
    "            S = len(arrs)\n",
    "            stacked = np.stack(arrs, axis=1)\n",
    "\n",
    "            # indices: for each group draw, choose a (possibly different) sample per subject\n",
    "            idx = rng.integers(0, K, size=(num_draws, S))\n",
    "            # vectorized gather; average across subjects\n",
    "            draws = stacked[idx, np.arange(S)].mean(axis=1)\n",
    "            rec[f\"{p}_samples_group\"] = draws.astype(np.float32)\n",
    "        group_samples.append(rec)\n",
    "    return group_samples\n",
    "\n",
    "group_samples = aggregate_group_mean(subj_deltas, num_draws=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd15323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing HDIs\n",
    "\n",
    "def compute_hdi_from_group(group_samples, hdi_prob=0.99):\n",
    "    rows = []\n",
    "    for ds in group_samples:\n",
    "        dsid = ds[\"dataset_id\"]\n",
    "        rec = {\"dataset_id\": dsid}\n",
    "        for p in [\"v\",\"a\",\"z\",\"ter\"]:\n",
    "            s = ds.get(f\"{p}_samples_group\")\n",
    "            if s is None or len(s)==0:\n",
    "                rec.update({f\"{p}_diff_mean\": np.nan,\n",
    "                            f\"{p}_diff_median\": np.nan,\n",
    "                            f\"{p}_hdi_lower\": np.nan,\n",
    "                            f\"{p}_hdi_upper\": np.nan,\n",
    "                            f\"{p}_excludes_zero\": False})\n",
    "                continue\n",
    "            lo = np.percentile(s, (1-hdi_prob)/2*100)\n",
    "            hi = np.percentile(s, (1+hdi_prob)/2*100)\n",
    "            rec.update({\n",
    "                f\"{p}_diff_mean\": float(np.mean(s)),\n",
    "                f\"{p}_diff_median\": float(np.median(s)),\n",
    "                f\"{p}_hdi_lower\": float(lo),\n",
    "                f\"{p}_hdi_upper\": float(hi),\n",
    "                f\"{p}_excludes_zero\": not (lo <= 0 <= hi),\n",
    "            })\n",
    "        rows.append(rec)\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe425baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdi_df = compute_hdi_from_group(group_samples, hdi_prob=0.99)\n",
    "hdi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ca76e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "fig, axes = plt.subplots(3, 2, figsize=(14, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "param_names = [\"v\", \"a\", \"z\", \"ter\"]\n",
    "\n",
    "for i, param in enumerate(param_names):\n",
    "    ax = axes[i]\n",
    "        \n",
    "    subset = hdi_df[[\"dataset_id\", \n",
    "                    f\"{param}_diff_mean\", \n",
    "                    f\"{param}_hdi_lower\", \n",
    "                    f\"{param}_hdi_upper\", \n",
    "                    f\"{param}_excludes_zero\"]].copy()\n",
    "\n",
    "    subset = subset.sort_values(\"dataset_id\")\n",
    "\n",
    "    for _, row in subset.iterrows():\n",
    "        y = row[\"dataset_id\"]\n",
    "        color = \"tab:blue\" if row[f\"{param}_excludes_zero\"] else \"gray\"\n",
    "        ax.plot([row[f\"{param}_hdi_lower\"], row[f\"{param}_hdi_upper\"]], [y, y], color=color)\n",
    "        ax.plot(row[f\"{param}_diff_mean\"], y, \"o\", color=color)\n",
    "\n",
    "    ax.axvline(0, color=\"black\", linestyle=\"--\")\n",
    "    ax.set_xlabel(\"ΔB-A\")\n",
    "    ax.set_ylabel(\"Dataset\")\n",
    "    ax.set_title(f\"Δ{param}\")\n",
    "    ax.set_yticks(range(1, 15))\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "for j in range(len(param_names), 6):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6071e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing BFs\n",
    "\n",
    "PRIOR_BOUNDS = {\n",
    "    \"vL\":   (0.0, 3.0),\n",
    "    \"vR\":   (0.0, 3.0),\n",
    "    \"a\":    (0.5, 2.5),\n",
    "    \"z\":    (0.05, 0.95),\n",
    "    \"terL\": (0.2, 0.6),\n",
    "    \"terR\": (0.2, 0.6),\n",
    "}\n",
    "\n",
    "def subject_counts(subj_deltas):\n",
    "    S = {}\n",
    "    for row in subj_deltas:\n",
    "        S[row[\"dataset_id\"]] = S.get(row[\"dataset_id\"], 0) + 1\n",
    "    return S\n",
    "\n",
    "def moment_matched_prior_sd_for_mu(S, num_draws=20000, seed=123, bounds=PRIOR_BOUNDS):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    def U(name): lo, hi = bounds[name]; return rng.uniform(lo, hi, size=(num_draws, S))\n",
    "    vL_A, vR_A = U(\"vL\"), U(\"vR\"); vL_B, vR_B = U(\"vL\"), U(\"vR\")\n",
    "    a_A, a_B   = U(\"a\"), U(\"a\")\n",
    "    z_A, z_B   = U(\"z\"), U(\"z\")\n",
    "    tL_A, tR_A = U(\"terL\"), U(\"terR\"); tL_B, tR_B = U(\"terL\"), U(\"terR\")\n",
    "    vA, vB   = vL_A+vR_A, vL_B+vR_B\n",
    "    aA, aB   = a_A, a_B\n",
    "    zA, zB   = np.log(1 - z_A), np.log(1 - z_B)\n",
    "    terA,terB= tL_A+tR_A,      tL_B+tR_B\n",
    "    dV, dA, dZ, dT = (vB-vA), (aB-aA), (zB-zA), (terB-terA)\n",
    "    muV, muA, muZ, muT = dV.mean(1), dA.mean(1), dZ.mean(1), dT.mean(1)\n",
    "    return {\"v\": float(muV.std(ddof=1)),\n",
    "            \"a\": float(muA.std(ddof=1)),\n",
    "            \"z\": float(muZ.std(ddof=1)),\n",
    "            \"ter\": float(muT.std(ddof=1))}\n",
    "\n",
    "def build_model_prior_sd_map(subj_deltas, num_draws=20000, seed=123, bounds=PRIOR_BOUNDS):\n",
    "    S_map = subject_counts(subj_deltas)\n",
    "    return {dsid: moment_matched_prior_sd_for_mu(S, num_draws=num_draws, seed=seed, bounds=bounds)\n",
    "            for dsid, S in S_map.items()}\n",
    "\n",
    "\n",
    "from scipy.stats import norm, gaussian_kde\n",
    "\n",
    "def compute_bf_from_group(group_samples, prior_sd=1.0, bw_mult=1.2, floor=1e-300):\n",
    "    \"\"\"\n",
    "    Simple Savage–Dickey BF at μ=0 using Normal(0, prior_sd) and KDE for posterior.\n",
    "    prior_sd can be scalar, per-param dict, or {dsid: {param: sd}}.\n",
    "    Returns BF10, log BF10.\n",
    "    \"\"\"\n",
    "    def _sd_for(dsid, p):\n",
    "        if isinstance(prior_sd, dict):\n",
    "            if dsid in prior_sd:\n",
    "                return float(prior_sd[dsid][p])\n",
    "            if p in prior_sd:\n",
    "                return float(prior_sd[p])\n",
    "        return float(prior_sd)\n",
    "\n",
    "    rows, tiny = [], np.finfo(float).tiny\n",
    "    for ds in group_samples:\n",
    "        dsid = ds[\"dataset_id\"]; rec = {\"dataset_id\": dsid}\n",
    "        for p in [\"v\",\"a\",\"z\",\"ter\"]:\n",
    "            s = ds.get(f\"{p}_samples_group\")\n",
    "            if s is None or len(s)==0:\n",
    "                rec.update({f\"{p}_bf_10\": np.nan, f\"{p}_ln_bf_10\": np.nan, f\"{p}_log10_bf_10\": np.nan})\n",
    "                continue\n",
    "\n",
    "            s = np.asarray(s, np.float64)\n",
    "            kde = gaussian_kde(s)\n",
    "            try: kde.set_bandwidth(kde.factor * bw_mult)\n",
    "            except Exception: pass\n",
    "            post0 = max(float(kde.evaluate(0.0)[0]), floor)\n",
    "\n",
    "            sd = max(_sd_for(dsid, p), tiny)\n",
    "            prior0 = float(norm(0, sd).pdf(0.0))\n",
    "\n",
    "            log_bf  = np.log(prior0) - np.log(post0)\n",
    "            bf10   = np.exp(log_bf) if log_bf < 700 else np.inf\n",
    "\n",
    "            rec[f\"{p}_bf_10\"]       = float(bf10)\n",
    "            rec[f\"{p}_log_bf_10\"]   = float(log_bf)\n",
    "            rec[f\"{p}_prior_sd\"]    = sd\n",
    "        rows.append(rec)\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b126a3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model-based σ_μ map from training priors (depends on S per dataset)\n",
    "model_sd_map = build_model_prior_sd_map(subj_deltas, num_draws=20000, seed=123)\n",
    "\n",
    "bf_df = compute_bf_from_group(group_samples, prior_sd=model_sd_map,\n",
    "                                     bw_mult=1.0, floor=1e-300)\n",
    "\n",
    "bf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9be0704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codebook of valid inferences, based on Dutilh et al. (2019)\n",
    "ground_truth_planned_analysis = [\n",
    "    {\"dataset_id\": 1, \"manipulated\": {}},\n",
    "    {\"dataset_id\": 2, \"manipulated\": {\"v\": \"B\"}},\n",
    "    {\"dataset_id\": 3, \"manipulated\": {\"a\": \"B\"}},\n",
    "    {\"dataset_id\": 4, \"manipulated\": {\"z\": \"B\"}},\n",
    "    {\"dataset_id\": 5, \"manipulated\": {\"v\": \"B\", \"a\": \"B\"}},\n",
    "    {\"dataset_id\": 6, \"manipulated\": {\"v\": \"B\", \"z\": \"B\"}},\n",
    "    {\"dataset_id\": 7, \"manipulated\": {\"a\": \"B\", \"z\": \"B\"}},\n",
    "    {\"dataset_id\": 8, \"manipulated\": {\"v\": \"A\", \"a\": \"B\"}},\n",
    "    {\"dataset_id\": 9, \"manipulated\": {\"v\": \"A\", \"z\": \"B\"}},\n",
    "    {\"dataset_id\": 10, \"manipulated\": {\"a\": \"A\", \"z\": \"B\"}},\n",
    "    {\"dataset_id\": 11, \"manipulated\": {\"v\": \"A\", \"a\": \"B\", \"z\": \"B\"}},\n",
    "    {\"dataset_id\": 12, \"manipulated\": {\"v\": \"B\", \"a\": \"A\", \"z\": \"B\"}},\n",
    "    {\"dataset_id\": 13, \"manipulated\": {\"v\": \"B\", \"a\": \"B\", \"z\": \"A\"}},\n",
    "    {\"dataset_id\": 14, \"manipulated\": {\"v\": \"B\", \"a\": \"B\", \"z\": \"B\"}},\n",
    "]\n",
    "\n",
    "ground_truth_alt_analysis_1 = [\n",
    "    {'dataset_id': 1, 'manipulated': {}},\n",
    "    {'dataset_id': 2, 'manipulated': {'v': 'B'}},\n",
    "    {'dataset_id': 3, 'manipulated': {'a': 'B', 'v': 'B'}},\n",
    "    {'dataset_id': 4, 'manipulated': {'z': 'B'}},\n",
    "    {'dataset_id': 5, 'manipulated': {'v': 'B', 'a': 'B'}},\n",
    "    {'dataset_id': 6, 'manipulated': {'v': 'B', 'z': 'B'}},\n",
    "    {'dataset_id': 7, 'manipulated': {'a': 'B', 'z': 'B', 'v': 'B'}},\n",
    "    {'dataset_id': 9, 'manipulated': {'v': 'A', 'z': 'B'}},\n",
    "    {'dataset_id': 10, 'manipulated': {'a': 'A', 'z': 'B', 'v': 'A'}},\n",
    "    {'dataset_id': 13, 'manipulated': {'v': 'B', 'a': 'B', 'z': 'A'}},\n",
    "    {'dataset_id': 14, 'manipulated': {'v': 'B', 'a': 'B', 'z': 'B'}}\n",
    "]\n",
    "\n",
    "ground_truth_alt_analysis_2 = [\n",
    "    {'dataset_id': 1, 'manipulated': {}},\n",
    "    {'dataset_id': 2, 'manipulated': {'v': 'B'}},\n",
    "    {'dataset_id': 3, 'manipulated': {'a': 'B', 'ter': 'B'}},\n",
    "    {'dataset_id': 4, 'manipulated': {'z': 'B'}},\n",
    "    {'dataset_id': 5, 'manipulated': {'v': 'B', 'a': 'B', 'ter': 'B'}},\n",
    "    {'dataset_id': 6, 'manipulated': {'v': 'B', 'z': 'B'}},\n",
    "    {'dataset_id': 7, 'manipulated': {'a': 'B', 'z': 'B', 'ter': 'B'}},\n",
    "    {'dataset_id': 8, 'manipulated': {'v': 'A', 'a': 'B', 'ter': 'B'}},\n",
    "    {'dataset_id': 9, 'manipulated': {'v': 'A', 'z': 'B'}},\n",
    "    {'dataset_id': 10, 'manipulated': {'a': 'A', 'z': 'B', 'ter': 'A'}},\n",
    "    {'dataset_id': 11, 'manipulated': {'v': 'A', 'a': 'B', 'z': 'B', 'ter': 'B'}},\n",
    "    {'dataset_id': 12, 'manipulated': {'v': 'B', 'a': 'A', 'z': 'B', 'ter': 'A'}},\n",
    "    {'dataset_id': 13, 'manipulated': {'v': 'B', 'a': 'B', 'z': 'A', 'ter': 'B'}},\n",
    "    {'dataset_id': 14, 'manipulated': {'v': 'B', 'a': 'B', 'z': 'B', 'ter': 'B'}}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4a565f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_direction(hdi_df, bf_df, bf_threshold=3.0, params=(\"v\",\"a\",\"z\",\"ter\")):\n",
    "    \"\"\"\n",
    "    Calls a condition difference when:\n",
    "      - 99% HDI excludes 0, AND\n",
    "      - BF10 > bf_threshold.\n",
    "    Direction is by the sign of the median Δ = (B - A):\n",
    "      Δ > 0 -> \"B\", Δ < 0 -> \"A\", Δ = 0 -> \"0\".\n",
    "    Calls \"0\" (no difference) when:\n",
    "      - HDI includes 0 AND BF10 < 1/bf_threshold.\n",
    "    Otherwise, \"0\" (inconclusive).\n",
    "    \"\"\"\n",
    "    merged = pd.merge(hdi_df, bf_df, on=\"dataset_id\", suffixes=(\"_hdi\", \"_bf\"))\n",
    "    rows = []\n",
    "    inv_thr = 1.0 / bf_threshold\n",
    "\n",
    "    for _, r in merged.iterrows():\n",
    "        dsid = r[\"dataset_id\"]\n",
    "        for p in params:\n",
    "            hdi_excl = bool(r.get(f\"{p}_excludes_zero\"))\n",
    "            bf = r.get(f\"{p}_bf_10\")\n",
    "            med = r.get(f\"{p}_diff_median\")\n",
    "\n",
    "            if hdi_excl and (bf is not None) and (bf > bf_threshold):\n",
    "                inferred = \"B\" if med > 0 else (\"A\" if med < 0 else \"0\")\n",
    "            elif (not hdi_excl) and (bf is not None) and (bf < inv_thr):\n",
    "                inferred = \"0\"\n",
    "            else:\n",
    "                inferred = \"0\"\n",
    "\n",
    "            rows.append({\n",
    "                \"dataset_id\": dsid,\n",
    "                \"param\": p,\n",
    "                \"hdi_excludes_zero\": hdi_excl,\n",
    "                \"hdi_B-A_median\": med,\n",
    "                \"bf_10\": bf,\n",
    "                \"inferred_direction\": inferred,\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd42457",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferred_df = infer_direction(hdi_df, bf_df)\n",
    "\n",
    "# Return the inference table\n",
    "inferred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f02648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def inference_evaluation(inference_df, ground_truth):\n",
    "    \"\"\"\n",
    "    Returns rates per parameter (and a Total row) with only:\n",
    "      - Correct  (correct directional + correct null)\n",
    "      - Miss     (called 0 but truth A/B)\n",
    "      - False Alarm (called A/B but truth 0)\n",
    "      - Flip     (called A vs truth B or vice versa)\n",
    "    \"\"\"\n",
    "    counts = defaultdict(lambda: {\"correct_dir\": 0, \"correct_null\": 0,\n",
    "                                  \"miss\": 0, \"false_alarm\": 0, \"flip\": 0, \"total\": 0})\n",
    "\n",
    "    # Lookup predictions\n",
    "    pred = {(r[\"dataset_id\"], r[\"param\"]): r[\"inferred_direction\"]\n",
    "            for _, r in inference_df.iterrows()}\n",
    "\n",
    "    for gt in ground_truth:\n",
    "        ds = gt[\"dataset_id\"]\n",
    "        truth_map = gt.get(\"manipulated\", {})\n",
    "        for p in [\"v\", \"a\", \"z\", \"ter\"]:\n",
    "            truth = truth_map.get(p, \"0\")\n",
    "            guess = pred.get((ds, p))\n",
    "            if guess is None:\n",
    "                continue\n",
    "\n",
    "            if truth == \"0\" and guess == \"0\":\n",
    "                counts[p][\"correct_null\"] += 1\n",
    "            elif truth in (\"A\",\"B\") and guess == truth:\n",
    "                counts[p][\"correct_dir\"] += 1\n",
    "            elif truth in (\"A\",\"B\") and guess == \"0\":\n",
    "                counts[p][\"miss\"] += 1\n",
    "            elif truth == \"0\" and guess in (\"A\",\"B\"):\n",
    "                counts[p][\"false_alarm\"] += 1\n",
    "            elif truth in (\"A\",\"B\") and guess in (\"A\",\"B\") and guess != truth:\n",
    "                counts[p][\"flip\"] += 1\n",
    "\n",
    "            counts[p][\"total\"] += 1\n",
    "\n",
    "    # Per-parameter rates (only requested columns)\n",
    "    rows = []\n",
    "    for p, c in counts.items():\n",
    "        T = c[\"total\"]\n",
    "        if T == 0:\n",
    "            rows.append({\"parameter\": p,\n",
    "                         \"Correct\": np.nan, \"Miss\": np.nan,\n",
    "                         \"False Alarm\": np.nan, \"Flip\": np.nan})\n",
    "            continue\n",
    "        correct = (c[\"correct_dir\"] + c[\"correct_null\"]) / T\n",
    "        rows.append({\"parameter\": p,\n",
    "                     \"Correct\": correct,\n",
    "                     \"Miss\": c[\"miss\"] / T,\n",
    "                     \"False Alarm\": c[\"false_alarm\"] / T,\n",
    "                     \"Flip\": c[\"flip\"] / T})\n",
    "\n",
    "    # Total row\n",
    "    tot = {\"correct_dir\": 0, \"correct_null\": 0, \"miss\": 0, \"false_alarm\": 0, \"flip\": 0, \"total\": 0}\n",
    "    for c in counts.values():\n",
    "        for k in tot: tot[k] += c[k]\n",
    "    TT = tot[\"total\"]\n",
    "    total_row = {\"parameter\": \"Total\"}\n",
    "    if TT == 0:\n",
    "        total_row.update({\"Correct\": np.nan, \"Miss\": np.nan, \"False Alarm\": np.nan, \"Flip\": np.nan})\n",
    "    else:\n",
    "        total_row.update({\n",
    "            \"Correct\": (tot[\"correct_dir\"] + tot[\"correct_null\"]) / TT,\n",
    "            \"Miss\": tot[\"miss\"] / TT,\n",
    "            \"False Alarm\": tot[\"false_alarm\"] / TT,\n",
    "            \"Flip\": tot[\"flip\"] / TT\n",
    "        })\n",
    "\n",
    "    return pd.concat([pd.DataFrame(rows), pd.DataFrame([total_row])], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94770f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "planned_eval = inference_evaluation(inferred_df, ground_truth_planned_analysis)\n",
    "planned_eval.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb96a4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_eval_1 = inference_evaluation(inferred_df, ground_truth_alt_analysis_1)\n",
    "alt_eval_1.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4411a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_eval_2 = inference_evaluation(inferred_df, ground_truth_alt_analysis_2)\n",
    "alt_eval_2.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f9d2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out non-decision time inferences\n",
    "inferred_df_no_ter = inferred_df[inferred_df[\"param\"] != \"ter\"].copy()\n",
    "\n",
    "# Evaluate with non-decision time excluded\n",
    "eval_no_ter = inference_evaluation(inferred_df_no_ter, ground_truth_planned_analysis)\n",
    "eval_no_ter.round(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
